---
title: "Analysis paper"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
options(knitr.kable.NA = '')
library(covid.german.forecasts)
library(here)
library(magrittr)
library(scoringutils)
library(knitr)
library(kableExtra)
library(data.table)
library(ggplot2)
library(ggridges)
library(RColorBrewer)
library(patchwork)
library(stringr)
```

```{r helper-functions}
# classify epidemic according to whether, on a given forecast date, the two
# weeks before that have seen monotonic rise, decline, or an unclear trend
classify_epidemic <- function(data, cutoff = 0.05, growth_cutoff = 2) {
  dt <- as.data.table(data)
  
  # calculate differences and set differences smaller than 
  # a certain cutoff to zero
  dt[, diff_1 := c(NA, diff(true_value, 1)), 
     by = c("location_name", "target_type")]
  dt[abs(diff_1) < (cutoff * true_value), diff_1 := 0]
  dt[, diff_prev := c(shift(diff_1, 1)), 
     by = c("location_name", "target_type")]
  
  # assign a label depending on observed differences
  dt[, c("classification", "speed") := "unclear"]
  dt[(diff_1 >= 0 & diff_prev >= 0), 
     classification := "increasing"]
  dt[(diff_1 <= 0 & diff_prev <= 0 ), 
     classification := "decreasing"]
  dt[(diff_1 == 0 & diff_prev == 0), 
     classification := "unclear"]
  dt[(classification == "increasing") & 
       diff_1 > growth_cutoff * diff_prev, 
     speed := "accelerating"]
    dt[(classification == "increasing") & 
         diff_1 < 1/growth_cutoff * diff_prev, 
     speed := "decelerating"]
  dt[(classification == "decreasing") & 
       abs(diff_1) > growth_cutoff * abs(diff_prev), 
     speed := "accelerating"]
  dt[(classification == "decreasing") & 
       abs(diff_1) < 1/growth_cutoff * abs(diff_prev), 
     speed := "decelerating"]
  
  dt[, c("diff_1", "diff_prev") := NULL]
  return(dt)
}

```


```{r}
# load and set up data
cases <- combined_data[target_type == "case" & 
                         as.Date(forecast_date) %in% forecast_dates$cases]
deaths <- combined_data[target_type == "death" & 
                         as.Date(forecast_date) %in% forecast_dates$deaths]
data <- rbindlist(list(cases, deaths))
data[, location_target := paste0(target_type, "s", " in ", location_name)]

# obtain classification
epitrend <- classify_epidemic(truth_data)
epitrend[, forecast_date := target_end_date + 2]
epitrend[, true_value := NULL]

```

## Todo

- [ ] final decision on the Christmas period. Do we want to also exclude target end dates, or just forceast dates? Do we want to exclude the 2nd of January?
- [ ] look more closely into the stratification: do we want the current thing or do we want something else that has the outcome in it as well? One thing I thought was have another category: inflection or no inflection and also score that?
- [ ] Talk to Johannes about whether we want to score log forecasts and log truth values instead. I for now removed that part for the following reasons: I currently don't have a good feeling for what it does. I'm somewhat hesitant that establishing a completely new way of scoring forecasts could be beyond the scope of this paper. Very happy to switch to it entirely if we agree it is the better metric, but not sure we are at that point yet. 

## Visualsiation of daily data

```{r}
dailytruth_data %>%
  dplyr::filter(target_end_date >= min(forecast_dates$full_hub_period), 
                target_end_date <= max(forecast_dates$full_hub_period)) %>%
  ggplot(aes(y = true_value, x = target_end_date)) + 
  geom_point() + 
  geom_line() + 
  facet_grid(target_type ~ location_name, scales = "free")

ggsave(here("analysis", "plots", "daily_truth.png"))
```
## Number of expert forecasters

```{r num-forecasters, results = 'asis'}
dt <- crowdforecast_data[!(model %in% c("Crowd-Rt-Forecast",
                                        "EpiNow2_secondary", 
                                        "EpiExpert-ensemble", 
                                        "EpiNow2")), 
                         .(`number of forecasters` = length(unique(model))), , 
                         by = c("forecast_date", "location_name", "target_type")
][order(forecast_date)][
  !is.na(forecast_date)]


dt[, .(sd = sd(`number of forecasters`), 
                             mean = mean(`number of forecasters`), 
                             min  = min(`number of forecasters`), 
                             max = max(`number of forecasters`), 
                             median = median(`number of forecasters`)), 
   by = c("location_name", "target_type")] 



```
```{r, results = 'asis'}
dt <- crowdforecast_data[!(model %in% c("Crowd-Rt-Forecast",
                                        "EpiNow2_secondary", 
                                        "EpiExpert-ensemble", 
                                        "EpiNow2")) & forecast_date %in% forecast_dates$unfiltered]

num_fc <- dt[, .(n_forecasts = length(unique(forecast_date))),
             by = c("model")][order(n_forecasts)]

num_fc %>%
  ggplot(aes(x = n_forecasts)) + 
  geom_bar() + 
  theme_light() + 
  labs(x = "Number of available submissions", 
       y = "Number of forecasters") 

ggsave("plots/crowdforecast_regularity.png")

```

## Number of expert forecasters

```{r, results = 'asis'}

dt <- crowdforecast_data[!(model %in% c("Crowd-Rt-Forecast",
                                        "EpiNow2_secondary", 
                                        "EpiNow2")) & forecast_date %in% forecast_dates$unfiltered]

crowddata <- merge_pred_and_obs(dt, truth_data)

scores <- eval_forecasts(
  data = crowddata[grepl("2", target)],
  summarise_by = c("model", "target_type"), 
  compute_relative_skill = TRUE, 
  baseline = "EpiExpert-ensemble"
)

plot_df <- merge(num_fc, scores, by = "model")

plot_df %>%
  ggplot(aes(y = scaled_rel_skill, x = n_forecasts)) + 
  geom_point() + 
  theme_light() + 
  facet_wrap(~ target_type) + 
  labs(y = "Scaled relative skill", x = "Number of available submissions") + 
  geom_smooth(method='lm', formula= y~x)

ggsave("plots/skill-vs-forecast-freq.png")


```

## Visualisation of 2 week ahead forecasts and scores

```{r truth-vs-forecasts, fig.height=10, fig.width=15}
# load unfiltered combined data 
df <- combined_data[as.Date(forecast_date) %in% forecast_dates$unfiltered &
                      grepl("2", target) & 
                      quantile %in% c(0.025, 0.25, 0.75, 0.975)]
df[, location_target := str_to_title(paste0(target_type, "s", " in ", location_name))]
             
df <- dcast(df, ... ~ quantile, value.var = "prediction")

# add trend classification
df <- merge(df, epitrend, 
            by = c("target_end_date", "location_name", "target_type"))

truth_vs_forecast_2 <- df %>%
  ggplot(aes(y = true_value, x = target_end_date)) + 
  # geom_linerange(aes(ymin = `0.025`, ymax = `0.975`, color = model), 
  #                size = 1.5,
  #                alpha = 0.4,
  #                position = position_dodge(width = 3)) + 
  geom_linerange(aes(ymin = `0.25`, ymax = `0.75`, color = model), 
                 size = 1.5,
                 alpha = 1,
                 position = position_dodge(width = 6)) + 
  labs(y = "value", x = "date") + 
  geom_point(aes(shape = classification)) + 
  geom_line() + 
  geom_rect(aes(xmin = min(forecast_dates$christmas) + 14, # 2wk ahead forecast
                xmax = max(forecast_dates$christmas) + 14,
                ymin = -Inf,
                ymax = Inf), fill = "lightgrey", alpha = 0.002) +
  theme_light() + 
  facet_wrap(~ location_target, scales = "free", ncol = 1) + 
  theme(legend.position = "bottom") + 
  scale_color_brewer(palette = "Accent") + 
  theme(panel.background = element_rect(fill = "#e1f0fe"))

ggsave(here("analysis", "plots", "truth-and-2-wk-ahead.png"), 
       plot = truth_vs_forecast_2)

```

```{r scores-over-time, fig.height=10, fig.width=15}
# plot with scores over time ---------------------------------------------------
scores_target_forecastdate <- eval_forecasts(
  data = data[grepl("2", target)],
  summarise_by = c("model", "location_name", "target_type", "forecast_date", "location_target"), 
  compute_relative_skill = TRUE, 
  baseline = "Baseline"
)

# add epi trend classification
scores_target_forecastdate[, forecast_date := as.Date(forecast_date)]
scores_target_forecastdate <-  merge(scores_target_forecastdate, epitrend, 
            by = c("forecast_date", "location_name", "target_type"))

rel_skill_time_2 <- scores_target_forecastdate  %>%
  ggplot(aes(x = as.Date(forecast_date), y = scaled_rel_skill)) + 
  geom_line(aes(colour = model)) + 
  geom_point(aes(shape = classification, colour = model)) + 
  theme_light() + 
  facet_wrap( ~ location_target, ncol = 1, scales = "free_y") +
  theme(legend.position = "bottom") +
  labs(y = "relative skill", x = "date") + 
  geom_rect(aes(xmin = min(forecast_dates$christmas) + 14,
                xmax = max(forecast_dates$christmas) + 14,
                ymin = 0,
                ymax = Inf), fill = "lightgrey", alpha = 0.002) +
  scale_color_brewer(palette = "Accent") + 
  theme(panel.background = element_rect(fill = "#e1f0fe")) +
  scale_y_log10()

ggsave(here("analysis", "plots", "rel-skill-over-time.png"), 
       plot = rel_skill_time_2)
```

```{r fig.height=10, fig.width=15}
# glue two plots together
truth_vs_forecast_2 | rel_skill_time_2 +
  plot_layout(guides = "collect") & 
  plot_annotation(tag_levels = 'A') &
  theme(legend.position = 'bottom') 

# for some reason the A / B annotation doesn't work here

ggsave(here("analysis", "plots", "figure-forecasts-2.png"))
```
*Figure 1. Left: Visualisation of 2 week ahead forecasts against the true observed values. The shape indicates whether there has been a monotonic increase or decrease over the last two weeks, or an unclear trend. Forecasts made during the Christmas period (from 2020-12-19 until 2021-01-02) are grayed out. Right: Corresponding scaled relative skill scores for the forecasts shown on the left. Scaled relative skill scores can be thought of as 'improvement over the baseline model' (see Methods for details).*

**What we learn from this plot**

- severe overshooting for Renewal model
- all models are terrible around Christmas
- the artifacts around Christmas hit Renewal hardest
- the Christmas effect is much stronger in Germany than in Poland

**Todo**
- [ ] maybe remove baseline from relative skill plot
- [ ] maybe rearrange plots
- [ ] make plots nicer (legend, maybe point size on the left / remove either points or lines)


---

## Summary table with scores

```{r}
# create helper function to make table
make_score_table <- function(scores) {
  scores <- as.data.table(scores)
  setnames(scores, 
           old = c("model", "target_type", "interval_score", "aem", 
                   "relative_skill", "scaled_rel_skill", "sharpness", 
                   "underprediction", "overprediction", "bias", 
                   "50", "90"), 
           new = c("Model", "Target", "WIS", "Abs. error", 
                   "rel. skill", "Skill", "Sharpness", 
                   "Underpred.", "Overpred.", "Bias", 
                   "50%-Cov.", "90%-Cov."))
  
  scores <- scores[, lapply(.SD, FUN = function(x) {
    if (is.numeric(x)) {
      return(round(signif(x, 3), 2))
    } else {
      return(x)
    }
  })]
  
  scores <- scores[, .SD,  .SDcols = !c("coverage_deviation", "rel. skill")][
    order(Target, `Skill`)
  ] 
  
  setcolorder(scores, 
              neworder = c("Model", "Target", "Skill"))
  
  # add an empty line to data.frame to separate deaths and cases
  table <- 
    rbindlist(list(
    scores[1:4], 
    as.list(rep(NA, ncol(scores))), 
    scores[5:9]
  )) %>%
    kable() %>% #format = "markdown"
    kable_styling(bootstrap_options = "striped")
  return(table)
}
```


### 2 weeks ahead
```{r }
# scores by target type for 2 week ahead forecasts

coverage <- eval_forecasts(
  data[grepl("2", target)], 
  summarise_by = c("model", "target_type", "range"), 
  compute_relative_skill = TRUE, 
  baseline = "Baseline"
)[range %in% c(50, 90), 
                   .(model, target_type, coverage, range)] %>%
  dcast(formula = ... ~ range, value.var = "coverage")

table <- eval_forecasts(
  data[grepl("2", target)], 
  summarise_by = c("model", "target_type"), 
  compute_relative_skill = TRUE, 
  baseline = "Baseline"
) %>%
  merge(coverage, all.x = TRUE, by = c("model", "target_type")) %>%
  make_score_table() 


save_kable(table, file = here("analysis", "plots", "table_scores_2_ahead.png"))

```

*Table 1: Scores for 2 week ahead forecasts (excluding the 21st and 28th of December 2020, cut to three significant digits and rounded). Skill is the scaled relative skill, a measure of relative performance with respect to the baseline model. Sharpness, overprediction and underprediction together some up to the weighted interval score (WIS). Bias (between -1 and 1) represents the general average tendency of a model to over- or underpredict. 50% and 90%-coverage are the percentage of observed values that fell within the 50% and 90% prediction intervals of a model.*

---

## Analyse distribution of scores

```{r}
# scores by target type for 4 week ahead forecasts
scores <- eval_forecasts(
  data[grepl("2", target)], 
  summarise_by = c("model", "target_type", "location_name", "forecast_date", 
                   "location_target"), 
  compute_relative_skill = TRUE, 
  baseline = "Baseline"
)[model != "Baseline"] 

scores[, forecast_date := as.Date(forecast_date)]
scores <-  merge(scores, epitrend, 
            by = c("forecast_date", "location_name", "target_type"))

```

```{r}
p <- ggplot(scores, aes(y = model, x = scaled_rel_skill, 
                          group = model, fill = model)) + 
  ggridges::geom_density_ridges() + 
  geom_vline(aes(xintercept = 1), alpha = 0.4) + 
  scale_fill_manual(values = brewer.pal(name = "Accent", n = 5)[2:5]) + 
  theme(panel.background = element_rect(fill = "#e1f0fe")) +
  facet_wrap( ~ location_target) + 
  theme(legend.position = "bottom") + 
  labs(y = "Scaled relative skill") + 
  scale_x_log10(labels = scales::number_format(accuracy = 0.1), 
                limits = c(0.015, 50))
```

```{r}
# scores for different phases of the epidemic
scores[, target_phase := str_to_title(paste0(target_type, "s - ", 
                                             classification, " phase"))]
scores[, target_phase := factor(target_phase, 
                                levels = str_to_title(c("Cases - decreasing phase", 
                                                        "Cases - unclear phase", 
                                                        "Cases - increasing phase", 
                                                        "Deaths - decreasing phase", 
                                                        "Deaths - unclear phase", 
                                                        "Deaths - increasing phase")))]

scores[, .(target_count = .N), by = c("target_phase")]

p_phases <- ggplot(scores, aes(y = model, x = scaled_rel_skill, 
                          group = model, fill = model)) + 
  ggridges::geom_density_ridges(quantile_lines = TRUE, quantiles = 0.5) + 
  geom_vline(aes(xintercept = 1), alpha = 0.4) + 
  scale_fill_manual(values = brewer.pal(name = "Accent", n = 5)[2:5]) + 
  theme(panel.background = element_rect(fill = "#e1f0fe")) +
  facet_wrap( ~ target_phase) + 
  theme(legend.position = "bottom") + 
  labs(y = "Scaled relative skill") + 
  scale_x_log10(labels = scales::number_format(accuracy = 0.1), 
                limits = c(0.015, 50))
```


```{r fig.height=10, fig.width=15}

p / p_phases + 
  plot_layout(guides = "collect") & 
  plot_annotation(tag_levels = 'A') &
  theme(legend.position = 'bottom') 

ggsave("plots/distribution_scores.png")

```

*Figure 2. Top: Distribution of the scaled relative skill scores for the different model and forecast targets. The vertical black line represents the baseline model. Models beat the baseline more easily for death forecasts. The renewal model had by far the most variance in terms of its performance, while the crowd forecasting model seems most consistent. Bottom: Distribution of scaled relative skill in different phases of the epidemic. The epidemic was classified as 'increasing', if the forecast point and the two data points before form a monotonically rising line, 'decreasing' if numbers were falling and 'unclear' otherwise. This plot seems like a lot of noise to me, but unsure.*



---

## WIS contributions
```{r}
df <- data[grepl("2", target)] %>%
  merge(epitrend, 
        by = c("target_end_date", "location_name", "target_type"))

scores_target <- eval_forecasts(
  df, 
  summarise_by = c("model", "target_type", "classification"), 
  compute_relative_skill = TRUE
) 

scores_target[, target_phase := paste0(target_type, "s - ", 
                                       classification, " phase")]

wis_components(scores_target, 
               facet_formula = ~ target_phase, 
               scales = "free_x", 
               relative_contributions = TRUE,
               x_text_angle = 0) + 
  theme(legend.position = "bottom") + 
  coord_flip() + 
  labs(x = NULL)

ggsave(here("analysis", "plots", "wis-components.png"))
  
```

*Figure 3. Relative contributions of sharpness, overprediction and underprediction to the weighted interval scores. Models overpredict in a decreasing phase and underpredict in a decreasing phase. So far so obvious. Somehow adding inflection points would be really cool - maybe stratify according to "decreasing, but inflection afterwards etc.?". Apparently the baseline model is really good in an unclear phase, just much too uncertain?.* 

---

```{r, eval = TRUE}
scores <- eval_forecasts(
  data[, horizon := substring(target, 1, 1)], 
  summarise_by = c("model", "target_type", "location_name", "horizon"), 
  compute_relative_skill = TRUE, 
  baseline = "Baseline"
)

scores %>%
  ggplot(aes(y = interval_score, x = horizon, color = model, 
             group = model)) + 
  geom_point() + 
  geom_line() + 
  theme(panel.background = element_rect(fill = "#e1f0fe")) +
  scale_color_brewer(palette = "Accent") + 
  scale_y_log10() + 
  facet_grid(target_type ~ location_name, scales = "free") + 
  theme(legend.position = "bottom")

```

```{r, eval = TRUE}
scores_full <- eval_forecasts(
  data[, horizon := substring(target, 1, 1)], 
  summarise_by = c("model", "location_target", "horizon", "forecast_date"), 
  compute_relative_skill = FALSE, 
  baseline = "Baseline"
)

means <- scores_full[, .(meanval = mean(interval_score), 
                         medianval = median(interval_score)), 
                     by =c("model", "location_target", "horizon")]

scores_full %>%
  ggplot(aes(y = interval_score, x = horizon, 
             fill = model, color = model)) +
  geom_boxplot(outlier.shape = 21, coef = 1.5, 
               width=0.5, alpha = 1, 
               position = position_dodge2(width = 0.5)) + 
  # geom_line(data = means, 
  #           aes(group = model, y = meanval, color = model), 
  #           position = position_dodge2(width = 0.5), 
  #           size = 2) +
  theme(panel.background = element_rect(fill = "#e1f0fe")) +
  scale_color_brewer(palette = "Accent") + 
  geom_point(data = means, 
             mapping = aes(y = meanval, fill = model),
             color = "black",
             # fill = "red",
             shape = 21,
             size = 2,
             position = position_dodge2(width = 0.5)) + 
    geom_point(data = means, 
             mapping = aes(y = medianval, fill = model),
             color = "black",
             # fill = "red",
             shape = 22,
             size = 2,
             position = position_dodge2(width = 0.5)) + 
  scale_fill_brewer(palette = "Accent") + 
  scale_y_log10() + 
  facet_wrap( ~ location_target, scales = "free") + 
  theme(legend.position = "bottom")

ggsave("plots/scores_horizons.png")

```

---

## Calibration

```{r, eval = TRUE}
df <- data[grepl("2", target)] %>%
  merge(epitrend, 
        by = c("target_end_date", "location_name", "target_type"))

scores_target <- eval_forecasts(
  df, 
  summarise_by = c("model", "target_type", "classification", "range", "quantile"), 
  compute_relative_skill = FALSE, 
  pit_plots = TRUE
) 

interval_coverage(scores_target, 
                  facet_formula = ~ target_type + classification)

```

  *Figure 4: Comparison of nominal and empirical coverage (percentage of true observed values covered by a given prediction interval). The one thing I find interesting about this plot is that models are apparently much too confident when there is an increasing trend. I feel this is probably actionable advice (even though it could probably be made better using a separate analysis about optimal uncertainty that is beyond the scope of this paper). *

**Todo**

- maybe make this a PIT plot

---

## Supplementary Material


### Look at the effect of data exclusions

- look at first period, second period etc
```{r, eval = FALSE}
scores <- list()
# scores for restricted data set
scores_full <- eval_forecasts(data, 
                              summarise_by = c("model", "target_type", "target"), 
                              compute_relative_skill = TRUE, 
                              baseline = "Baseline")

scores[["full"]] <- 
  scores_full[, .(model, target_type, scaled_rel_skill, interval_score, target,
                  scenario = "full")]

res <- rbindlist(scores)
res %>%
  ggplot(aes(x = scenario, y = scaled_rel_skill, color = model, group = model)) + 
  geom_point() + 
  facet_wrap( ~ target, scales = "free", ncol = 2) + 
  theme(legend.position = "bottom")
```






<!-- **Further things we could look at** -->

<!-- - maybe look at correlation between the number of weekly cases and the score? -->
<!-- - maybe it would be interesting to look at other baseline models, e.g. "how hard is it to beat a model that just always continues the trend?" -->
<!-- - look at crowd performance as a function of the number of forecasts --> maybe leave that to a second paper -->
