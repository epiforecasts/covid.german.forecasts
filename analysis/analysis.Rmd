---
title: "Analysis paper"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
library(covid.german.forecasts)
library(here)
library(magrittr)
library(scoringutils)
library(knitr)
library(kableExtra)
library(data.table)
library(ggplot2)
library(ggridges)
library(RColorBrewer)
library(patchwork)

# load and set up data
cases <- combined_data[target_type == "case" & 
                         as.Date(forecast_date) %in% forecast_dates$cases]
deaths <- combined_data[target_type == "death" & 
                         as.Date(forecast_date) %in% forecast_dates$deaths]
data <- rbindlist(list(cases, deaths))
data[, location_target := paste0(target_type, "s", " in ", location_name)]
```

## Visualisation of data


```{r fig.height=10, fig.width=15}
df <- combined_data[as.Date(forecast_date) %in% forecast_dates$unfiltered &
                      grepl("2", target) & 
                      quantile %in% c(0.025, 0.25, 0.75, 0.975)]
             
truth <- dcast(df, ... ~ quantile, value.var = "prediction")[
  , .(location_name, target_end_date, target_type, true_value)] %>%
  unique()

truth <- truth[order(target_type, location_name, target_end_date)]


# classify epidemic according to whether, on a given forecast date, the two
# weeks before that have seen monotonic rise, decline, or an unclear trend
classify_epidemic <- function(data, cutoff = 0.05, growth_cutoff = 2) {
  dt <- as.data.table(data)
  
  # calculate differences and set differences smaller than 
  # a certain cutoff to zero
  dt[, diff_1 := c(NA, diff(true_value, 1)), 
     by = c("location_name", "target_type")]
  dt[abs(diff_1) < (cutoff * true_value), diff_1 := 0]
  dt[, diff_prev := c(shift(diff_1, 1)), 
     by = c("location_name", "target_type")]
  
  # assign a label depending on observed differences
  dt[, c("classification", "speed") := "unclear"]
  dt[(diff_1 >= 0 & diff_prev >= 0), 
     classification := "increasing"]
  dt[(diff_1 <= 0 & diff_prev <= 0 ), 
     classification := "decreasing"]
  dt[(diff_1 == 0 & diff_prev == 0), 
     classification := "unclear"]
  dt[(classification == "increasing") & 
       diff_1 > growth_cutoff * diff_prev, 
     speed := "accelerating"]
    dt[(classification == "increasing") & 
         diff_1 < 1/growth_cutoff * diff_prev, 
     speed := "decelerating"]
  dt[(classification == "decreasing") & 
       abs(diff_1) > growth_cutoff * abs(diff_prev), 
     speed := "accelerating"]
  dt[(classification == "decreasing") & 
       abs(diff_1) < 1/growth_cutoff * abs(diff_prev), 
     speed := "decelerating"]
  
  dt[, c("diff_1", "diff_prev") := NULL]
  return(dt)
}

dt <- classify_epidemic(truth)

p <- ggplot(dt, aes(x = target_end_date)) + 

  geom_line(aes(y = true_value)) + 
  geom_point(aes(y = true_value, color = classification)) + 
  labs(y = "value", x = "date") + 
  theme_light() + 
  facet_grid(target_type ~ location_name, scales = "free") + 
  theme(legend.position = "bottom")

show_acceleration <- FALSE
if (show_acceleration) {
  p <- p + 
    geom_rect(aes(xmin = target_end_date - 7,
                  xmax = target_end_date,
                  ymin = -Inf,
                  ymax = Inf,
                  fill = speed), alpha = 0.2) +
    scale_fill_manual(values = c(
      "accelerating" = "darkgrey",
      "decelerating" = "lightgrey",
      "unclear" = "white"
    ))
}

print(p)


```



## Visualisation of forecasts and scores

This includes all available dates for the forecast plot. For the score plot it includes the full set for cases, and a restricted set for deaths. The restricted set is all forecasts from "2020-12-07" on to make sure that we have a full set of forecasts for deaths. The plot also includes the Christmas period - it probably makes sense to gray that out. 

```{r fig.height=10, fig.width=15}
df <- combined_data[as.Date(forecast_date) %in% forecast_dates$unfiltered &
                      grepl("2", target) & 
                      quantile %in% c(0.025, 0.25, 0.75, 0.975)]
df[, location_target := paste0(target_type, "s", " in ", location_name)]
             
df <- dcast(df, ... ~ quantile, value.var = "prediction")

df <- classify_epidemic(df)

truth_vs_forecast_2 <- df %>%
  ggplot(aes(y = true_value, x = target_end_date)) + 
  # geom_linerange(aes(ymin = `0.025`, ymax = `0.975`, color = model), 
  #                size = 1.5,
  #                alpha = 0.4,
  #                position = position_dodge(width = 3)) + 
  geom_linerange(aes(ymin = `0.25`, ymax = `0.75`, color = model), 
                 size = 1.5,
                 alpha = 1,
                 position = position_dodge(width = 6)) + 
  labs(y = "value", x = "date") + 
  geom_point(aes(shape = classification)) + 
  geom_line() + 
  theme_light() + 
  facet_wrap(~ location_target, scales = "free", ncol = 1) + 
  theme(legend.position = "bottom") + 
  scale_color_brewer(palette = "Accent") + 
  theme(panel.background = element_rect(fill = "#e1f0fe"))

ggsave(here("analysis", "plots", "truth-and-2-wk-ahead.png"), 
       plot = truth_vs_forecast_2)

```

```{r fig.height=10, fig.width=15}
# plot with scores over time ---------------------------------------------------
scores_target_forecastdate <- eval_forecasts(
  data = data[grepl("2", target)],
  summarise_by = c("model", "location_name", "target_type", "forecast_date", "location_target"), 
  compute_relative_skill = TRUE, 
  baseline = "Baseline"
)
truth <- classify_epidemic(truth)
tmp <- copy(truth)
tmp[, target_end_date := as.Date(target_end_date) + 2]
scores_target_forecastdate[, forecast_date := as.Date(forecast_date)]
setnames(tmp, old = "target_end_date", new = "forecast_date")
scores_target_forecastdate <- merge(
  scores_target_forecastdate, tmp, 
  by = c("location_name", "forecast_date", "target_type"), 
  all.x = TRUE)

wis_scores_time_2 <- scores_target_forecastdate  %>%
  ggplot(aes(x = as.Date(forecast_date), y = interval_score, colour = model)) + 
  geom_line() + 
  geom_point() + 
  theme_light() + 
  facet_wrap(~ location_target, scales = "free", ncol = 1) +
  theme(legend.position = "bottom") +
  labs(y = "weighted interval score", x = "date") +
  scale_color_brewer(palette = "Accent") + 
  theme(panel.background = element_rect(fill = "#e1f0fe"))

ggsave(here("analysis", "plots", "wis-over-time.png"), 
       plot = wis_scores_time_2)
```


```{r}
### same plot with mean score ratio
rel_skill_time_2 <- scores_target_forecastdate  %>%
  ggplot(aes(x = as.Date(forecast_date), y = scaled_rel_skill, fill = model, colour = model)) + 
  geom_line() + 
  geom_point(aes(shape = classification)) + 
  theme_light() + 
  facet_wrap( ~ location_target, ncol = 1, scales = "free") +
  theme(legend.position = "bottom") +
  labs(y = "relative skill", x = "date") +
  scale_color_brewer(palette = "Accent") + 
  theme(panel.background = element_rect(fill = "#e1f0fe"))

ggsave(here("analysis", "plots", "rel-skill-over-time.png"), 
       plot = rel_skill_time_2)
```

```{r fig.height=10, fig.width=15}
# glue two plots together
truth_vs_forecast_2 | rel_skill_time_2 + 
  plot_layout(guides = "collect") & 
  theme(legend.position = 'bottom') 
ggsave(here("analysis", "plots", "figure-forecasts-2.png"))
```

**What we learn from this plot**

- severe overshooting for Renewal model
- all models are terrible around Christmas
- the artifacts around Christmas hit Renewal hardest
- the Christmas effect is much stronger in Germany than in Poland
- need to increase value for position dodge
- reporting artifacts around christmas

**Todo**

- [ ] shade dates that are excluded in "official scoring"
- [ ] maybe add density plot with scores on the y axis
- [ ] remove baseline from relative skill plot
- [ ] make both plots nicer
    - [ ] increase dodge
    - [ ] change colour scheme
    - [ ] adjust size of points
- [ ] presumably change how plots are arranged
- [ ] make final decision whether we want to have relative skill or just WIS on the other plot
    - WIS would show the overall level (which scales with the number of cases)
    - relative skill corrects for that in some sene, but also introduces other biases

**Further things we could look at**

- maybe look at correlation between the number of weekly cases and the score?
- look at scores for rising and falling parts of the epidemic separately
- maybe it would be interesting to look at other baseline models, e.g. "how hard is it to beat a model that just always continues the trend?"


---

## Summary table with scores

This excludes all forecasts made on c("2020-12-21", "2020-12-28"). 

```{r}
# create helper function to make table
make_score_table <- function(scores) {
  scores <- as.data.table(scores)
  setnames(scores, 
           old = c("target_type", "interval_score", "aem", "relative_skill", "scaled_rel_skill"), 
           new = c("target", "WIS", "absolute error", "rel. skill", "scaled rel. skill"))
  
  table <- scores[, .SD,  .SDcols = !c("coverage_deviation", "rel. skill")][
    order(target, `scaled rel. skill`)
  ] %>%
    kable(format = "markdown")
  return(table)
}
```


### 2 weeks ahead
```{r }
# scores by target type for 2 week ahead forecasts

coverage <- eval_forecasts(
  data[grepl("2", target)], 
  summarise_by = c("model", "target_type", "range"), 
  compute_relative_skill = TRUE, 
  baseline = "Baseline"
)[range %in% c(50, 90), 
                   .(model, target_type, coverage, range)] %>%
  dcast(formula = ... ~ range, value.var = "coverage")
setnames(coverage, old = c("50", "90"), new = c("coverage 50%", "coverage 90%"))

eval_forecasts(
  data[grepl("2", target)], 
  summarise_by = c("model", "target_type"), 
  compute_relative_skill = TRUE, 
  baseline = "Baseline"
) %>%
  merge(coverage, all.x = TRUE, by = c("model", "target_type")) %>%
  make_score_table()
```

### 4 weeks ahead
```{r }
# scores by target type for 4 week ahead forecasts
coverage <- eval_forecasts(
  data[grepl("4", target)], 
  summarise_by = c("model", "target_type", "range"), 
  compute_relative_skill = TRUE, 
  baseline = "Baseline"
)[range %in% c(50, 90), 
                   .(model, target_type, coverage, range)] %>%
  dcast(formula = ... ~ range, value.var = "coverage")
setnames(coverage, old = c("50", "90"), new = c("coverage 50%", "coverage 90%"))

eval_forecasts(
  data[grepl("4", target)], 
  summarise_by = c("model", "target_type"), 
  compute_relative_skill = TRUE, 
  baseline = "Baseline"
) %>%
  merge(coverage, all.x = TRUE, by = c("model", "target_type")) %>%
  make_score_table()

```

**To do**

- [] make a final decision (and document it somewhere) about how we want to exclude the Christmas period. Right now we are excluding forecast dates, but do not restrict target end dates in any way. 

**Further things to look at**
- maybe think about some kind of stratification by time? 

---

## Look more closely at relative skill

--> not sure what we wanted to do here. 

---

## Analyse distribution of model rankings

```{r }
# scores by target type for 4 week ahead forecasts
scores <- eval_forecasts(
  data[grepl("2", target)], 
  summarise_by = c("model", "target_type", "location_name", "forecast_date")
) 

ranked_scores <- scores %>%
  dplyr::mutate(forecast_date = as.Date(forecast_date)) %>%
  dplyr::group_by(forecast_date, location_name, target_type) %>%
  dplyr::mutate(num_forecasts = dplyr::n(), 
                rank = rank(interval_score, ties.method = "average",
                                           na.last = NA), 
                standard_rank = round((1 - (rank - 1) / (num_forecasts - 1)) * 100)) %>%
  dplyr::ungroup()

setDT(ranked_scores)
df <- ranked_scores[, .(rank_count = .N), 
                    by = c("target_type", "location_name", "model", "rank")][, 
                      sum := .N, by = c("target_type", "location_name")
                    ][, rank_count := rank_count / sum]

ggplot(df, aes(y = model, x = rank, 
                          group = model, fill = model)) + 
  ggridges::geom_ridgeline(aes(height = rank_count)) + 
  facet_wrap(location_name ~ target_type, scales = "free_x")

ggplot(ranked_scores, aes(y = model, x = rank, 
                          group = model, fill = model)) + 
  ggridges::geom_density_ridges(stat = "binline") + 
  facet_wrap(location_name ~ target_type, scales = "free_x")

```

### Model rankings in terms of scaled relative skill 

```{r }
# scores by target type for 4 week ahead forecasts
scores <- eval_forecasts(
  data[grepl("2", target)], 
  summarise_by = c("model", "target_type", "location_name", "forecast_date"), 
  compute_relative_skill = TRUE, 
  baseline = "Baseline"
)[model != "Baseline"] 

ggplot(scores, aes(y = model, x = (scaled_rel_skill), 
                          group = model, fill = model)) + 
  ggridges::geom_density_ridges() + 
  geom_vline(aes(xintercept = 1), alpha = 0.4) + 
  facet_wrap(location_name ~ target_type, scales = "free_x")

```

**what we learn from this plot**

- bimodal distribution is interesting

palette = 
**To do**

- [ ] make plot nicer
- [ ] maybe make this with relative skill instead
- [ ] finish this todo list
- [ ] maybe a density plot instead of a ridge plot? 




---

## Look at WIS contributions
```{r}
scores_target <- eval_forecasts(
  data[grepl("2", target)], 
  summarise_by = c("model", "target_type"), 
  compute_relative_skill = TRUE
) 

wis_components(scores_target, 
               facet_formula = ~ target_type, 
               scales = "free_x", 
               relative_contributions = TRUE,
               x_text_angle = 0) + 
  theme(legend.position = "bottom") + 
  coord_flip() + 
  labs(title = "Components of the weighted interval score for 2-week ahead forecasts", 
       x = NULL)

ggsave(here("analysis", "plots", "wis-components.png"))

scores_target <- eval_forecasts(
  data[grepl("4", target)], 
  summarise_by = c("model", "location_name", "target_type"), 
  compute_relative_skill = TRUE
) 

wis_components(scores_target, 
               facet_formula = location_name ~ target_type, 
               facet_wrap_or_grid = "grid",
               scales = "free_x", 
               x_text_angle = 0) + 
  theme(legend.position = "bottom") + 
  coord_flip() + 
  labs(title = "Components of the weighted interval score for 2-week ahead forecasts", 
       x = NULL) 
  
```

**Todo**

- [ ] have the second plot stratified by epidemic period

---

## Look more closely at interventions and change points

- one interesting point Johannes brought up is that scoring predictions aroud change points may not even be such a good idea. The reason is that if you have a modle that always predicts a change point, theat model will do very well according to this metric even if it doesn't add much value. For example, we would expect the baseline model to do very well according to this change point metric

---

## Look at the effect of data exclusions

- look at first period, second period etc
```{r, eval = FALSE}
scores <- list()
# scores for restricted data set
scores_full <- eval_forecasts(data, 
                              summarise_by = c("model", "target_type", "target"), 
                              compute_relative_skill = TRUE, 
                              baseline = "Baseline")

scores[["full"]] <- 
  scores_full[, .(model, target_type, scaled_rel_skill, interval_score, target,
                  scenario = "full")]

res <- rbindlist(scores)
res %>%
  ggplot(aes(x = scenario, y = scaled_rel_skill, color = model, group = model)) + 
  geom_point() + 
  facet_wrap( ~ target, scales = "free", ncol = 2) + 
  theme(legend.position = "bottom")
```

---

## Look more closely at calibration and sharpness

```{r, eval = TRUE}
scores_target <- eval_forecasts(
  data[grepl("2", target)], 
  summarise_by = c("model", "target_type", "range", "quantile"), 
  compute_relative_skill = FALSE
) 

interval_coverage(scores_target, 
                  facet_formula = ~ target_type)

scores_target <- eval_forecasts(
  data[grepl("2", target)], 
  summarise_by = c("model", "location_name", "target_type", "range", "quantile"), 
  compute_relative_skill = FALSE
) 

interval_coverage(scores_target, 
                  facet_formula = location_name ~ target_type, 
                  facet_wrap_or_grid = "grid")

```

**What we learn from this plot**

- models are widely overconfident in their predictive ability
- more overconfidence with cases than deaths


**Todo**

- maybe make tihs a PIT plot
- stratify by period


---

## Look at scores based on log predictions and log truth values
```{r, eval = TRUE}

log_data <- copy(data)[, `:=`(true_value = log(true_value), 
                              prediction = log(prediction))]
  
```

### Table 2 weeks ahead
```{r, eval = TRUE}
log_scores_target_2 <- eval_forecasts(
  log_data[grepl("2", target)], 
  summarise_by = c("model", "target_type"), 
  compute_relative_skill = TRUE, 
  baseline = "Baseline"
) 

log_scores_target_2 %>%
  make_score_table()
```


### Table 2 weeks ahead
```{r, eval = TRUE}
log_scores_target_4 <- eval_forecasts(
  log_data[grepl("4", target)], 
  summarise_by = c("model", "target_type"), 
  compute_relative_skill = TRUE, 
  baseline = "Baseline"
) 

log_scores_target_4 %>%
  make_score_table()
```


**What we learn**
- results and rankings change if we look at scores of log values
- this makes sense as presumably outliers aren't punished so harshly (inconsistent? models with large outliers are treated more leniently)

### scores for logged values over time
```{r, eval = TRUE}
log_scores_target_forecast_date_2 <- eval_forecasts(
  log_data[grepl("2", target)], 
  summarise_by = c("model", "target_type", "forecast_date", "location_name"), 
  compute_relative_skill = TRUE, 
  baseline = "Baseline"
) 

rel_skill_time_2 <- log_scores_target_forecast_date_2  %>%
  ggplot(aes(x = as.Date(forecast_date), y = scaled_rel_skill, colour = model)) + 
  geom_line() + 
  geom_point() + 
  theme_light() + 
  facet_grid(target_type ~ location_name, scales = "free") +
  theme(legend.position = "bottom") +
  labs(y = "relative skill", x = "date") 

rel_skill_time_2
```

**what we learn**
- nothing? 
- it looks slightly different, but similar and also similarly confusing

### WIS components for scores of logged values
```{r, eval = TRUE}
log_scores_target_location_2 <- eval_forecasts(
  log_data[grepl("2", target)], 
  summarise_by = c("model", "target_type", "location_name"), 
  compute_relative_skill = TRUE, 
  baseline = "Baseline"
) 

wis_components(log_scores_target_location_2, 
               facet_formula = location_name ~ target_type, 
               facet_wrap_or_grid = "grid",
               scales = "free_x", 
               x_text_angle = 0) + 
  theme(legend.position = "bottom") + 
  coord_flip() + 
  labs(title = "Components of the weighted interval score for 2-week ahead forecasts", 
       x = NULL) 
```
**what we learn**

- the Renewal model benefits from this scoring scheme (except for Polish deaths)
- interesting (confusing?) that the Baseline model is now the worst model for Polish cases. What's the mechanism behind that? is it just bad on average, but has no outliers?

**To do**

- at least for me (Nikos): get an intuition for what actually changes when you look at scores of logged predictions and what does it imply. What kind of model benefits from one or the other evaluation scheme? 

Thoughts about what changes when you look at logged predictions

- models with outliers benefit
- do models lose out that aren't great, but have no outliers? 


## Further interesting things

- look at crowd performance as a function of the number of forecasts --> maybe leave that to a second paper