---
title: "analysis-graveyard"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
library(covid.german.forecasts)
library(here)
library(magrittr)
library(scoringutils)
library(knitr)
library(kableExtra)
library(data.table)
library(ggplot2)
library(ggridges)
library(RColorBrewer)
library(patchwork)
```

```{r helper-functions}
# classify epidemic according to whether, on a given forecast date, the two
# weeks before that have seen monotonic rise, decline, or an unclear trend
classify_epidemic <- function(data, cutoff = 0.05, growth_cutoff = 2) {
  dt <- as.data.table(data)
  
  # calculate differences and set differences smaller than 
  # a certain cutoff to zero
  dt[, diff_1 := c(NA, diff(true_value, 1)), 
     by = c("location_name", "target_type")]
  dt[abs(diff_1) < (cutoff * true_value), diff_1 := 0]
  dt[, diff_prev := c(shift(diff_1, 1)), 
     by = c("location_name", "target_type")]
  
  # assign a label depending on observed differences
  dt[, c("classification", "speed") := "unclear"]
  dt[(diff_1 >= 0 & diff_prev >= 0), 
     classification := "increasing"]
  dt[(diff_1 <= 0 & diff_prev <= 0 ), 
     classification := "decreasing"]
  dt[(diff_1 == 0 & diff_prev == 0), 
     classification := "unclear"]
  dt[(classification == "increasing") & 
       diff_1 > growth_cutoff * diff_prev, 
     speed := "accelerating"]
    dt[(classification == "increasing") & 
         diff_1 < 1/growth_cutoff * diff_prev, 
     speed := "decelerating"]
  dt[(classification == "decreasing") & 
       abs(diff_1) > growth_cutoff * abs(diff_prev), 
     speed := "accelerating"]
  dt[(classification == "decreasing") & 
       abs(diff_1) < 1/growth_cutoff * abs(diff_prev), 
     speed := "decelerating"]
  
  dt[, c("diff_1", "diff_prev") := NULL]
  return(dt)
}

```



**Further things we could look at**

- maybe look at correlation between the number of weekly cases and the score?
- maybe it would be interesting to look at other baseline models, e.g. "how hard is it to beat a model that just always continues the trend?"



```{r fig.height=10, fig.width=15}
df <- combined_data[as.Date(forecast_date) %in% forecast_dates$unfiltered &
                      grepl("2", target) & 
                      quantile %in% c(0.025, 0.25, 0.75, 0.975)]
             
truth <- dcast(df, ... ~ quantile, value.var = "prediction")[
  , .(location_name, target_end_date, target_type, true_value)] %>%
  unique()

truth <- truth[order(target_type, location_name, target_end_date)]




dt <- classify_epidemic(truth)

p <- ggplot(dt, aes(x = target_end_date)) + 

  geom_line(aes(y = true_value)) + 
  geom_point(aes(y = true_value, color = classification)) + 
  labs(y = "value", x = "date") + 
  theme_light() + 
  facet_grid(target_type ~ location_name, scales = "free") + 
  theme(legend.position = "bottom")

show_acceleration <- FALSE
if (show_acceleration) {
  p <- p + 
    geom_rect(aes(xmin = target_end_date - 7,
                  xmax = target_end_date,
                  ymin = -Inf,
                  ymax = Inf,
                  fill = speed), alpha = 0.2) +
    scale_fill_manual(values = c(
      "accelerating" = "darkgrey",
      "decelerating" = "lightgrey",
      "unclear" = "white"
    ))
}

print(p)


```

``` {r}

wis_scores_time_2 <- scores_target_forecastdate  %>%
  ggplot(aes(x = as.Date(forecast_date), y = interval_score, colour = model)) + 
  geom_line() + 
  geom_point() + 
  theme_light() + 
  facet_wrap(~ location_target, scales = "free", ncol = 1) +
  theme(legend.position = "bottom") +
  labs(y = "weighted interval score", x = "date") +
  scale_color_brewer(palette = "Accent") + 
  theme(panel.background = element_rect(fill = "#e1f0fe"))

ggsave(here("analysis", "plots", "wis-over-time.png"), 
       plot = wis_scores_time_2)

``` 


``` {r}
scores <- eval_forecasts(
  data[grepl("2", target)], 
  summarise_by = c("model", "target_type", "location_name", "forecast_date")
) 

ranked_scores <- scores %>%
  dplyr::mutate(forecast_date = as.Date(forecast_date)) %>%
  dplyr::group_by(forecast_date, location_name, target_type) %>%
  dplyr::mutate(num_forecasts = dplyr::n(), 
                rank = rank(interval_score, ties.method = "average",
                                           na.last = NA), 
                standard_rank = round((1 - (rank - 1) / (num_forecasts - 1)) * 100)) %>%
  dplyr::ungroup()

setDT(ranked_scores)
df <- ranked_scores[, .(rank_count = .N), 
                    by = c("target_type", "location_name", "model", "rank")][, 
                      sum := .N, by = c("target_type", "location_name")
                    ][, rank_count := rank_count / sum]


ggplot(df, aes(y = model, x = rank, 
                          group = model, fill = model)) + 
  ggridges::geom_ridgeline(aes(height = rank_count)) + 
  facet_wrap(location_name ~ target_type, scales = "free_x")

ggplot(ranked_scores, aes(y = model, x = rank, 
                          group = model, fill = model)) + 
  ggridges::geom_density_ridges(stat = "binline") + 
  facet_wrap(location_name ~ target_type, scales = "free_x")

```



```{r}
scores_target <- eval_forecasts(
  data[grepl("4", target)], 
  summarise_by = c("model", "location_name", "target_type"), 
  compute_relative_skill = TRUE
) 

wis_components(scores_target, 
               facet_formula = location_name ~ target_type, 
               facet_wrap_or_grid = "grid",
               scales = "free_x", 
               x_text_angle = 0) + 
  theme(legend.position = "bottom") + 
  coord_flip() + 
  labs(title = "Components of the weighted interval score for 2-week ahead forecasts", 
       x = NULL) 
```



```{r}
scores_target <- eval_forecasts(
  data[grepl("2", target)], 
  summarise_by = c("model", "location_name", "target_type", "range", "quantile"), 
  compute_relative_skill = FALSE
) 

interval_coverage(scores_target, 
                  facet_formula = location_name ~ target_type, 
                  facet_wrap_or_grid = "grid")
```


## Part about scoring log predictions and log truth




## Look at scores based on log predictions and log truth values
```{r, eval = TRUE}

log_data <- copy(data)[, `:=`(true_value = log(true_value), 
                              prediction = log(prediction))]
  
```

### Table 2 weeks ahead
```{r, eval = TRUE}
log_scores_target_2 <- eval_forecasts(
  log_data[grepl("2", target)], 
  summarise_by = c("model", "target_type"), 
  compute_relative_skill = TRUE, 
  baseline = "Baseline"
) 

log_scores_target_2 %>%
  make_score_table()
```


### Table 2 weeks ahead
```{r, eval = TRUE}
log_scores_target_4 <- eval_forecasts(
  log_data[grepl("4", target)], 
  summarise_by = c("model", "target_type"), 
  compute_relative_skill = TRUE, 
  baseline = "Baseline"
) 

log_scores_target_4 %>%
  make_score_table()
```


**What we learn**
- results and rankings change if we look at scores of log values
- this makes sense as presumably outliers aren't punished so harshly (inconsistent? models with large outliers are treated more leniently)

### scores for logged values over time
```{r, eval = TRUE}
log_scores_target_forecast_date_2 <- eval_forecasts(
  log_data[grepl("2", target)], 
  summarise_by = c("model", "target_type", "forecast_date", "location_name"), 
  compute_relative_skill = TRUE, 
  baseline = "Baseline"
) 

rel_skill_time_2 <- log_scores_target_forecast_date_2  %>%
  ggplot(aes(x = as.Date(forecast_date), y = scaled_rel_skill, colour = model)) + 
  geom_line() + 
  geom_point() + 
  theme_light() + 
  facet_grid(target_type ~ location_name, scales = "free") +
  theme(legend.position = "bottom") +
  labs(y = "relative skill", x = "date") 

rel_skill_time_2
```

**what we learn**
- nothing? 
- it looks slightly different, but similar and also similarly confusing

### WIS components for scores of logged values
```{r, eval = TRUE}
log_scores_target_location_2 <- eval_forecasts(
  log_data[grepl("2", target)], 
  summarise_by = c("model", "target_type", "location_name"), 
  compute_relative_skill = TRUE, 
  baseline = "Baseline"
) 

wis_components(log_scores_target_location_2, 
               facet_formula = location_name ~ target_type, 
               facet_wrap_or_grid = "grid",
               scales = "free_x", 
               x_text_angle = 0) + 
  theme(legend.position = "bottom") + 
  coord_flip() + 
  labs(title = "Components of the weighted interval score for 2-week ahead forecasts", 
       x = NULL) 
```
**what we learn**

- the Renewal model benefits from this scoring scheme (except for Polish deaths)
- interesting (confusing?) that the Baseline model is now the worst model for Polish cases. What's the mechanism behind that? is it just bad on average, but has no outliers?

**To do**

- at least for me (Nikos): get an intuition for what actually changes when you look at scores of logged predictions and what does it imply. What kind of model benefits from one or the other evaluation scheme? 

Thoughts about what changes when you look at logged predictions

- models with outliers benefit
- do models lose out that aren't great, but have no outliers? 