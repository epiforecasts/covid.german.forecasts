# Evaluating crowd sourced forecasts of Covid-19 against epidemiological model forecasts in Germany and Poland

*Nikos Bosse\*, Sam Abbott\*, and Sebastian Funk*

*\* contributed unequally*\*\*

\*\* *did they?*

Target journal: elife

Target completion of first draft: 26/03/2021

Target preprint date: 14/04/2021

## Abstract

#### Background

Forecasts generated by computer models and algorithms have played an important role in shaping public policy throughout the Covid-19 pandemic. The models, in turn, have been tweaked and shaped by human judgement. Any model forecast is therefore a mix between the researcher's subjective opinion and mechanistic model assumptions. 

#### Methods

To discern these two components we looked at forecasts Covid-19 case and death numbers submitted to the German and Polish Forecast Hub between October 2020 until February 2021. Crowd sourced human forecasts were compared against predictions by an untuned epidemiological baseline model from the same working group, as well as against the ensemble of all predictions submitted to the hub. 


#### Results

Human forecasts outperformed models (including the Forecast Hub ensemble) on case forecasts, but not on death forecasts. They performed best over longer time horizons and around Christmas where reporting artifacts had a major influence on the data. 

#### Conclusions

Expert opinion outperforms models at predicting long-term trends and when dealing with data anomalies, as humans can make use of information, e.g. about potential future policy interventions, not directly available to models. Computer models, however, have an edge in situations like XY and when modelling epidemiological parameters like the delay between current case numbers and future deaths. 

## Introduction

The COVID-19 pandemic has resulted in an increase of interest in infectious disease forecasting, and the evaluation of these forecasts. Single model forecasts (cite Imperial, cite US version of Imperial) were impactful on policy decisions early in the pandemic despite previous work having shown that relying on a single model can lead to less accurate forecasts than decisions based on multiple approaches (cite whatever hubs cite for this). Since then several collaborations have sort to evaluate Covid-19 forecasts in the United Kingdom (cite Seb), in the United States of America (cite Reich), and in Germany and Poland (cite Johannes). Whilst all of these efforts have successfully delivered more accurate forecasts to policy makers compared to individual forecasters efforts they have struggled to unpick what leads to good Covid-19 forecasts (cite all hub evaluations again). 

This has been partly driven by the complexity of the models used to produce the consituent forecasts but also because of the level of expert intervention in most forecasting methods over time, and in response to changes in the pandemic. These issues can be decoupled by separating infectious disease forecasting into "automatic" model derived forecasts and human elicitation forecasts (from now on referred to as crowd forecasts). Model based forecasts have a rich history and have been growing in popularity over the last decade (cite influenza hub, cite dengue challenge, cite something else). However, it is unsusual for "automated" real-time forecasts (as opposed to retrospective forecasts) to be evaluated with forecasts usually being submitted by individual researchers and therefore liable to change over time in response to percieved performance, changes in the underlying infectious disease processes or for other reasons. A variety of human expert elicitation as well as crowd forecasting projects exist [Cite Tom McAndrew, Metaculus, GoodJudgement, PredicIt]. However, these forecasts usually follow a different format than the ones provided by traditional forecasting models or take on different questions. Unlike these projects the crowd forecasts we develop here have been specifically designed to be comparable to model based forecasts. 


In this work, we evaluate two constrasting forecasting approaches that simplify and synthesis some these themes. The first of these approaches is a crowd forecast, where expert and non-expert opinion is synthesised into a single forecast of cases and deaths in target locations. This represents modellers interventions in forecasts but in a model agnostic format. In the second approach, we use two recently developed short term forecasting methods that make minimal epidemiological assumptions of how notifications are generated over time coupled with a robust observation model. These models were then not tuned throughout the submission period in order to make a comparison to opinion derived forecasts possible. All of these forecasts were submitted to the German/Poland forecast hub over XX weeks from the XX to the XX and combined, along with other forecasts, into an ensemble used by policy makers as well as being independently evaluated by the research group running the German/Poland forecasting hub.

## Methods

### Data sources

Data on test positive cases and deaths linked to Covid-19 were provided by the organisers of the German and Polish forecast hub (P/L hub) (cite German/Poland hub). For the first half of the evaluation period (XX to XX) these data were sourced from the European Centre for Disease Control(ECDC) (cite ECDC data) with data then being sourced from the Robert Koch Institute (RKI) for the remainder of the submission period. These data are subject to reporting artefacts (such as a retrospective case reporting in Poland on the 24th November (cite artefacts)), changes in reporting over time and variation in testing regimes.

Line list data used to inform the delay from symptom onset to test postive case report or death in the model based forecasts was sourced from (cite public linelist) with data available up to June (check exact date). Population data at the national and state level in Germany and Poland used in the model based forecasts was sourced from (source for population data). 
 
### Forecasts

#### Model based forecasts

We used two models from the `EpiNow2` R package (version 1.3.3) as our baseline model based forecasts [@epinow2]. These were chosen for their relatively simplisticty, attention to modelling the observation model of the forecast targets, and their grounding in simplistic epidemiological assumptions. The first of these models, which was used to forecast both test positive cases and deaths, used the renewal equation (cite EpiEstim paper) and an approximate Gaussian process [@approxGP] to estimate the effective reproduction number over time for latent infections and then convolved these infections to the target observation using data based delay distributions (cite EpiNow2, website and Kaths paper). The second model, which was only used to forecast deaths, assumed that deaths could be modelled using a scaling parameter, a convolution of test positive cases with a distribution that described the delay from case report to death, and a negative binomial observation model with a day of the week effect [@epinow2]. Both models are described in detail in the supplementary information. 

Each forecast target was fit independently for each model using Markov-chain Monte Carlo (MCMC) in stan [@rstan]. A minimum of 4 chains were used with a warmup of 250 samples for the renewal equation based model and 1000 samples for the convolution model. 2000 samples total post warmup were used for the renewal equation model and 4000 samples of the convolution model. Different settings were chosen for each model to optimise compute time contigent on convergence. Convergence was assessed using the R hat diagnostic [@rstan]. For the convolution model forecast the case forecast from the renewal equation model was used in place of observed cases beyond the forecast horizon using 1000 posterior samples. 


#### Crowd forecast

Crowd forecasts were created by ensembling forecasts submitted by individual participants. Participants were recruited mostly within the Centre of Mathematical Modeling of Infectious Diseases at the London School of Hygiene and Tropical Medicine, but participants were also invited personally or via social media to submit predictions. 

##### Collection

Participants were asked to make forecasts using a web application (https://cmmid-lshtm.shinyapps.io/crowd-forecast/) built using the shiny R package (cite shiny) and available in the `crowdforecaster` R package (cite crowdforecaster). In the application they could select a predictive distribution, with the default being log-normal, and adjust the median and the width of the uncertainty by either interacting with a figure showing their forecast or providing numerical values. The baseline shown was a repetition of the last known observation with some constant uncertainty around it based on changes observed in the data in the previous four weeks. We required that participants submitted forecasts with uncertainty that increased over time. Our interface also allowed participants to view the observed data, and their forecasts, using a log scale and presented additional contextual COVID-19 data sourced from our world in data (cite ourworldindata). These data included notifications of both test positive COVID-19 cases and COVID-19 linked deaths, case fatality rates and the number of COVID-19 tests though the availability of this data evolved over the study period. 


##### Processing

Forecasts were downloaded, cleaned and processed every week for submission. If a forecaster had submitted multiple predictions for a single target, only the latest submission was kept. Some personal information (like the exact time of the forecast) was removed. Information on the chosen distribution as well as the parameters for median and width were used to obtain a set of 23 quantiles from that distribution. Forecasts from all eligible forecasters were then aggregated using an unweighted quantile-wise mean. In the beginning, inclusion was decided based on the authors' ad-hoc assessment of the validitiy of the forecast submission. Almost all forecasts were kept if they weren't clearly a result of someone experimenting with the app. From XX we based inclusion on the criterion that a forecaster submitted forecasts for at least two targets. 


### Forecast submission

- When we submitted
- What we submitted
- How we submitted (Docker, R, R package, CRON, Azure).

Forecasts were submitted every Tuesday (using data up until Monday) for a one to four week ahead horizon. Forecasts were in a quantile-based formats with 22 quantiles plus the median prediction. 

Forecasts are available here:


### Statistical analysis
Forecasts were analysed by visual inspection as well formal model evaluation. Forecast submissions were visualised by forecast time horizon and compared to the ensemble of all forecasts from the German and Polish Forecast Hub. To evaluate model performance more formally we used the weighted interval score (wis) [@bracherEvaluatingEpidemicForecasts2021], as well as empirical coverage of the 50% and 90% prediction intervals. In order to obtain a model ranking, relative skill was assessed based on pairwise comparisons between all possible combinations of models. Scores were calculated using the `scoringutils` package [@scoringutils] in R. Dates with known reporting issues were excluded a priori from the evaluation. 

- Summarised forecast scores are presented across different horions (focus on weeks 1 and 2 like German hub). 
- Discuss outperformance at 4 week horizon with all 1 and 2 weeks repeated in the SI. Discuss performance in relation to the German/Poland hub ensemble.
- Split forecast time period into different states (stable/unstable?) and report short term model performance. 
- Pull out intervention dates in germany/poland which models did well/badly around these (we need to know the intervention dates to do this).

## Results

### Forecast submission

*This should be two concise paragraphs one about the first few points and one about the crowd forecast collection.*
- Submitted every week (number of weeks and number of forecasts). Comment that model based forecasts were also submitted at region level but not further analysised here as we could not also produce this number of expert forecasts.
- Interventions applied at different points throughtout the study period but not explicitly modelled.
- Model based forecasts used the same approach throughout the forecast period with no changes to the methodology or setting apart from the introduction of the convolution based model for forecasting deaths on the.
- Had X number of expert forecasters on average with min of and max of. Some comment on regularity of forecasters. 
- Changes over time to the interface for expert forecasts with improvements throughout the study period and a steady increase in participation.
    - added additional information from ourworldindata
    - changed visual appearance
    - small changes in usability (e.g. you can go back to your forecasts)


### Comparison of forecast submissions
![](https://i.imgur.com/mtsOau9.png)
![](https://i.imgur.com/jQPaD9Z.png)
*Figure 1. Top: Visualisation of two week ahead forecasts. Dark shaded bars indicate the 50% prediction interval, lighter shaded bars the 90% prediction interval. Bottom: Scores over time (Decide on what score to show)*

- Overview of what we found. Discuss figure 1 showing forecasts at 1 and 2 week horizon (3 and 4 in SI and referenced here). Compare submissions to each other and to the german hub visually.
- Table 1 forecast scores at 2 week time horizon. SI for 1 week and 4 week.
- Table 2 (or figure) scores by forecast time period (stable, unstabe, intervention/no intervention).
- Compare performance on deaths vs cases (think about which to lead with and how to present).

<table>
 <thead>
  <tr>
   <th style="text-align:left;"> model </th>
   <th style="text-align:left;"> target </th>
   <th style="text-align:right;"> WIS </th>
   <th style="text-align:right;"> sharpness </th>
   <th style="text-align:right;"> underprediction </th>
   <th style="text-align:right;"> overprediction </th>
   <th style="text-align:right;"> bias </th>
   <th style="text-align:right;"> absolute error </th>
   <th style="text-align:right;"> rel. skill </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:left;"> Hub-ensemble </td>
   <td style="text-align:left;"> case </td>
   <td style="text-align:right;"> 16270.5868 </td>
   <td style="text-align:right;"> 5400.75487 </td>
   <td style="text-align:right;"> 3507.22079 </td>
   <td style="text-align:right;"> 7362.61118 </td>
   <td style="text-align:right;"> -0.0147727 </td>
   <td style="text-align:right;"> 24178.0040 </td>
   <td style="text-align:right;"> 0.8670351 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> EpiExpert </td>
   <td style="text-align:left;"> case </td>
   <td style="text-align:right;"> 16332.9114 </td>
   <td style="text-align:right;"> 3633.33108 </td>
   <td style="text-align:right;"> 6395.15513 </td>
   <td style="text-align:right;"> 6304.42518 </td>
   <td style="text-align:right;"> -0.0554545 </td>
   <td style="text-align:right;"> 23618.9546 </td>
   <td style="text-align:right;"> 0.8703563 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> EpiNow2 </td>
   <td style="text-align:left;"> case </td>
   <td style="text-align:right;"> 24867.5363 </td>
   <td style="text-align:right;"> 5481.29245 </td>
   <td style="text-align:right;"> 5821.59387 </td>
   <td style="text-align:right;"> 13564.65000 </td>
   <td style="text-align:right;"> 0.1420455 </td>
   <td style="text-align:right;"> 33814.0909 </td>
   <td style="text-align:right;"> 1.3251536 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Hub-ensemble </td>
   <td style="text-align:left;"> death </td>
   <td style="text-align:right;"> 272.9964 </td>
   <td style="text-align:right;"> 112.83307 </td>
   <td style="text-align:right;"> 85.91470 </td>
   <td style="text-align:right;"> 74.24864 </td>
   <td style="text-align:right;"> -0.0445455 </td>
   <td style="text-align:right;"> 442.3246 </td>
   <td style="text-align:right;"> 0.7370936 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> EpiExpert </td>
   <td style="text-align:left;"> death </td>
   <td style="text-align:right;"> 337.9704 </td>
   <td style="text-align:right;"> 92.24881 </td>
   <td style="text-align:right;"> 130.41849 </td>
   <td style="text-align:right;"> 115.30306 </td>
   <td style="text-align:right;"> -0.0729545 </td>
   <td style="text-align:right;"> 525.7780 </td>
   <td style="text-align:right;"> 0.9101836 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> EpiNow2-convolution </td>
   <td style="text-align:left;"> death </td>
   <td style="text-align:right;"> 436.9801 </td>
   <td style="text-align:right;"> 98.11353 </td>
   <td style="text-align:right;"> 271.35780 </td>
   <td style="text-align:right;"> 67.50879 </td>
   <td style="text-align:right;"> -0.0821429 </td>
   <td style="text-align:right;"> 643.7679 </td>
   <td style="text-align:right;"> 1.1215692 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> EpiNow2 </td>
   <td style="text-align:left;"> death </td>
   <td style="text-align:right;"> 506.3914 </td>
   <td style="text-align:right;"> 143.67352 </td>
   <td style="text-align:right;"> 86.02273 </td>
   <td style="text-align:right;"> 276.69516 </td>
   <td style="text-align:right;"> 0.1481818 </td>
   <td style="text-align:right;"> 732.7273 </td>
   <td style="text-align:right;"> 1.3289916 </td>
  </tr>
</tbody>
</table>
*Table 1. Scores for 2 week ahead forecasts*


## Discussion

### Summary

- Expert opinion performed on average as well as or better than model derived, and expert tuned, forecasts.
- At short time horizons untuned models designed to perform well in real-time more rapidly adapted to state changes but this was offset but reduced long term improvement. 
- A simple model that assumed that cases were a convolution of deaths performed well compared to other approaches and outperformed expert opinion at short time horizons and did relatively well at longer time horizons.
- Optimising a forecast to a specific time horizon relies on understanding the use case of those consuming the forecasts.
- Crowd forecasts performed well but where difficult to recruit and hard to extend over a large number of forecast targets. Here for example we could not produce crowd forecasts at the state level in either Poland or Germany due to a lack of researcher time and our ability to reach out to potential forecasters.
- Something about how most of the participants were from the UK and therefore it as a) hard to motivate them for GM/PL and b) they didn't have a lot of domain expertise there

### Strengths and Weaknesses

*When writing this layer strengths and limitations together. Start with a strength and then for every limitation counter with a strength*

Our work has robustly assessed the performance of crowd-sourced human predictions and model based forecasts in a realistic real-time setting. Forecasts reflect unbiased predictive performance at the time and could not be tuned in response to reporting artifacts after submission as they were registered with an independent research organisation. Our evaluation followed a methodoloy pre-registered by the German and Polish Forecast Hub [@pre-registration] which makes sure our results can be fairly compared against offical forecast hub evaluations. 

While the methodology did not change for the Renewal model, the Convolution model and the Baseline model, this continuity is not given for the crowd forecasts and the hub ensemble model. Comparability of crowd forecasts at different time points is hampered by the low number of participants we were able to recruit initially and the fact that participants kept joining or dropping out. Similarly, the composition of the Hub ensemble changed over time as did many of the individual models contributing forecasts to the Forecast Hub. To mitigate this, a wide range of potential confounding factors, like different time periods, were considered to ensure the robustness of the obtained results. 

Human forecasters performed surprisingly well in spite of the low number of contributors. The fact that the interface demanded some understanding of distributions and time series made it hard to recruit participants who didn't already have a background in either forecasting or epidemiology. On the other hand, the setup allowed us to obtain a full predictive distribution instead of only a limited set of quantiles. This made it possible to compare expert forecasts directly against computer generated forecasts using the same evaluation tools. While forecasts performed well for a limited set of targets the setup is not easily scalable to a large set of prediction targets. Using R shiny also came with some limitations in terms of usability. On the other hand, setting the platform up as a public R package means that it can easily be adapted and re-used for future forecasting projects. 


- S: the R package is public and accessible for anyone to use

**Strengths and weaknesses in the context of the literature**

*Same structure as for the previous section.*

- Compare to US hub work (they had lots more data and many more models however there data source was very challenging and their models were all essentially black boxes to one degree or another making drawing conclusions difficult).
- Compare to Germany hub work (similar as above + different focus). Mention prespecified which is good.
- Compare to other model comparison efforts that don't include crowd forecasting.
- Compare to other crowd forecasting efforts. Examples to include: Delphi (as most comparable) and then binary prediction projects.

**Future work**

- Optimise the model based forecasts into a single submission for the ECDC based hub
- Compare the model based forecasts used here to simple time series based approaches and evaluate using weighted ensembles. Mention we are currently doing this in the US. 
- Expand crowd forecasting work to more targets and continue to improve/evaluate interface.
- Explore other crowd forecasting methods (i.e Rt based if not including here).

**Conclusions**

- Crowd forecasts outperform models with simplistic epidemiology derived assumptions at longer time horizons. 
- At shorter time horizons performance is more comparable, especially when forecasting deaths when a model that simplistically assumes that deaths are scaled convolution of cases performs relatively well.


# Supplementary information

## Forecast models

### Effective Reproduction number model

The model was initialised prior to the first observed data point by assuming constant exponential growth for the mean of assumed delays from infection to case report. 

\begin{align}
  I_{t} &= I_0 \exp  \left(r t \right)  \\
  I_0 &\sim \mathcal{LN}(\log I_{obs}, 0.2) \\
  r &\sim \mathcal{LN}(r_{obs}, 0.2) \\
\end{align}

Where $I_{obs}$ and $r_{obs}
- 12 weeks of data
- Prior log-normal with a mean of 1.1 and a standard deviation of 0.2
- Days with missing data or 0 notifications adjusted to the 7 day moving average if the 7 day moving average of notifications was greater than 50 per day. 
- Population adjustment (cite epidemia)
- Assumed static normally distributed reporting fraction with a mean of 0.25 and a standard deviation of 0.05 for test positive cases and a mean of 0.005 with a standard deviation of 0.0025 for COVID-19 linked deaths.
- Rt fixed from the forecast horizon.
- 4 chains, 250 warmup samples per chain, and 2000 samples overall post warmup.

We estimated the instantaneous reproduction number ($R_t$) using the `EpiNow2` R package (version 1.2.1) [@epinow2] on the last 12 weeks of available data 

The instantaneous reproduction number represents the number of secondary cases arising from an individual showing symptoms at a particular time, assuming that conditions remain identical after that time, and is therefore a measure of the instantaneous transmissibility (in contrast to the case reproduction number - see Fraser (2007) [@Fraser:2007hf] for a full discussion). `EpiNow2` implements a Bayesian latent variable approach using the probabilistic programming language Stan [@rstan], which works as follows. The initial number of infections were estimated as a free parameter with a prior based on the initial number of cases, or deaths, respectively. The initial, unobserved, growth rate was estimated from the first 7 days of reported data. This was used as a prior (normal with standard deviation 0.2) to estimate latent infections prior to the first reported case using a log linear model. For each subsequent time step, previous imputed infections ($I_{t-1}$) were summed, weighted by an uncertain generation time probability mass function ($w$), and combined with an estimate of $R_t$ to give the incidence at time $t$ ($I_t$) [@epinow2; @cori2013; @THOMPSON2019100356]. We used a log normal prior for the reproduction number ($R_0$) with mean 1 and standard deviation 0.2 reflecting our current belief that $R_t$ is likely to be centred around 1 in most of the world, with public health interventions and individual behaviour combining to prevent it from growing significantly larger for sustained periods. 

The infection trajectories were then mapped to mean reported case counts ($D_t$) by convolving over an uncertain incubation period and report delay distribution (convolved into $\xi$). Observed reported case counts ($C_t$) were then assumed to be generated from a negative binomial observation model with overdispersion $\phi$ (using 1 over the square root of a half normal prior with mean 1) and mean $D_t$, multiplied by a day of the week effect with an independent parameter for each day of the week ($\omega_{(t \mod 7)}$). Temporal variation was controlled using an approximate Gaussian process [@approxGP] with a squared exponential kernel ($GP$). In mathematical notation,


This package implements a Bayesian latent variable approach using the probabilistic programming language Stan (27). To initialise the model, infections were imputed prior to the first observed case using a log linear model with priors based on the first week of observed cases. This means that the initial observations both inform the initial parameters and are then also fit, which makes the initial Rt estimates less reliable than later estimates. This was a pragmatic choice to allow the model to be identifiable when only estimating part of the observed epidemic. We explored other parameterisations, but these suffered from poor model identification. For each subsequent time step with observed cases, new infections were imputed using the sum of previous modelled infections weighted by the generation time probability mass function, and combined with an estimate of Rt, to give the prevalence at time t (12). The generation time was assumed to follow a gamma distribution that was fixed over time but varied between samples, with priors drawn from the literature for the mean and standard deviation (28).

\begin{align}
  I_{t_{unobserved}} &= I_0 \exp  \left(r t_{unobserved}\right)  \\
  I_0 &\sim \mathcal{LN}(\log I_{observed}, 0.2) \\
  r &\sim \mathcal{LN}(r_{observed}, 0.2) \\
\end{align}


\begin{align}
  \log R_{t} &= \log R_{t-1} + \mathrm{GP}_t \\
  I_t &= R_t \sum_\tau^{15} w(\tau | \mu_{w}, \sigma_{w}) I_{t - \tau} \\
  O_t &= \sum_\tau^{15} \xi_{O}(\tau | \mu_{\xi_{O}}, \sigma_{\xi_{O}}) I_{t-\tau} \\
  D_t &= \alpha \sum_\tau^{15} \xi_{D}(\tau | \mu_{\xi_{D}}, \sigma_{\xi_{D}}) O_{t-\tau} \\ 
  C_t &\sim \mathrm{NB}\left(\omega_{(t \mod 7)}D_t, \phi\right)
\end{align}


Where,
\begin{align}
     R_0 &\sim \mathcal{LN}(0.079, 0.18) \\
     w &\sim \mathcal{G}(\mu_{w}, \sigma_{w}) \\
    \xi_{O} &\sim \mathcal{LN}(\mu_{\xi_{O}}, \sigma_{\xi_{O}}) \\
    \xi_{D} &\sim \mathcal{LN}(\mu_{\xi_{D}}, \sigma_{\xi_{D}}) \\
\end{align}

with the following priors, 

\begin{align}
    \mu_w &\sim \mathcal{N}(3.6, 0.7) \\
    \sigma_w &\sim \mathcal{N}(3.1, 0.8) \\
    \mu_{\xi_{O}} &\sim \mathcal{N}(1.62, 0.064) \\
    \sigma_{\xi_{O}} &\sim \mathcal{N}(0.418, 0.069) \\
    \mu_{\xi_{D}} &\sim \mathcal{N}(0.614, 0.066) \\
    \sigma_{\xi_{D}} &\sim \mathcal{N}(1.51, 0.048) \\
    \alpha &\sim \mathcal{N}(0.25, 0.05) \\
    \frac{\omega}{7} &\sim \mathrm{Dirichlet}(1, 1, 1, 1, 1, 1, 1) \\
    \phi &\sim \frac{1}{\sqrt{\mathcal{N}(0, 1)}}
\end{align}

When forecasting deaths the following alternative priors were used,

\begin{align}
    \mu_{\xi_{D}} &\sim \mathcal{N}(2.29, 0.076) \\
    \sigma_{\xi_{D}} &\sim \mathcal{N}(0.76, 0.055) \\
    \alpha &\sim \mathcal{N}(0.005, 0.0025) \\
\end{align}

$\alpha$, $\mu$, $\sigma$, and $\phi$ were truncated to be greater than 0 and with $\xi$, and $w$ normalised to sum to 1. $GP_t$ is an approximate Hilbert space gaussian process as defined in [@approxGP] using a Matern 3/2 kernel using a boundary factor of 1.5 and 17 basis functions (20% of the number of days used in fitting). The lengthscale of the Gaussian process was given a log-normal prior with a mean of 21 days, and a standard deviation of 7 days truncated to be greater than 3 days and less than 60 days. The magnitude of the Gaussian process was assumed be normally distributed centred at 0 with a standard deviation of 0.1. The prior for the generation time was sourced from [@generationinterval] but refit using a log-normal incubation period with a mean of 5.2 days (SD 1.1) and SD of 1.52 days (SD 1.1) with this incubation period also being used as a prior [@incubationperiod] for $\xi_{O}. This resulted in a gamma distributed generation time with mean 3.6 days (standard deviation (SD) 0.7), and SD of 3.1 days (SD 0.8) for all estimates. We estimated the delay between symyptom onset and case report or death required to convolve latent infections to observations by fitting an integer adjusted log-normal distribution to 10 subsampled bootstraps of a public linelist for cases in Germany from April 2020 to June 2020 with each bootstrap using 1% or 1769 samples of the available data  [@kraemer2020epidemiological; @covidregionaldata] and combining the posteriors for the mean and standard deviation of the log-normal distribution [@epinow2; @rt-website; @kath; @rstan]. This resulted in a delay distribution from symptom onset to case report with a mean of XX and a standard deviation of XX and a delay distribution from sypmtom onset to death with a mean of XX and a standard deviation of XX.


From the forecast time horizon ($T$) and onwards the last value of the Gaussian process was used (hence $R_t$ was assumed to be fixed) and latent infections were adjusted to account for the proportion of the population that was susceptible to infection as follows, 
 
\begin{equation}
    I_t = (N - I^c_{t-1}) \left(1 - \exp \left(\frac{-I'_t}{N - I^c_{T}}\right)\right),
\end{equation}

where $I^c_t = \sum_{s< t} I_s$ are cumulative infections by $t-1$ and $I'_t$ are the unadjusted infections defined above. This adjustment is based on that implemented in the `epidemia` R package (cite epidemia, cite @bhatt202).


Each forecast target was fit independently using using Markov-chain Monte Carlo (MCMC) in stan [@rstan]. A minimum of 4 chains were used with a warmup of 250 each and 2000 samples total post warmup. Convergence was assessed using the R hat diagnostic [@rstan].

We used an estimate of the generation time sourced from . 


### Convolution model



- Summarise key choices
- data used
- 
\begin{equation} 
    D_{t} \sim \mathrm{NB}\left(\omega_{(t \mod 7)} \alpha \sum_{\tau = 0}^{30} \xi(\tau | \mu, \sigma) C_{t-\tau},  \phi \right)
\end{equation}

Where,
\begin{align}
    \frac{\omega}{7} &\sim \mathrm{Dirichlet}(1, 1, 1, 1, 1, 1, 1) \\
    \alpha &\sim \mathcal{N}(0.01, 0.02) \\
    \xi &\sim \mathcal{LN}(\mu, \sigma) \\
    \mu &\sim \mathcal{N}(2.5, 0.5) \\
\sigma &\sim \mathcal{N}(0.47, 0.2) \\
\phi &\sim \frac{1}{\sqrt{\mathcal{N}(0, 1)}}
\end{align}


with $\alpha$, $\mu$, $\sigma$, and $\phi$ truncated to be greater than 0 and with $\xi$ normalised such that $\sum_{\tau = 0}^{30} \xi_(\tau | \mu, \sigma) = 1$. Only the last 3 weeks of data were included in the likelihood though all 12 weeks of data was used during fitting.

4 chains with 1000 warmup samples and 4000 posterior samples. 1000 posterior samples of the case forecast were then randomly matched with the posterior samples from the convolution model with the model being rerun for each sample to provide a forecast of future deaths. 


**4 week ahead forecasts**
<table>
 <thead>
  <tr>
   <th style="text-align:left;"> model </th>
   <th style="text-align:left;"> target </th>
   <th style="text-align:right;"> WIS </th>
   <th style="text-align:right;"> sharpness </th>
   <th style="text-align:right;"> underprediction </th>
   <th style="text-align:right;"> overprediction </th>
   <th style="text-align:right;"> bias </th>
   <th style="text-align:right;"> absolute error </th>
   <th style="text-align:right;"> rel. skill </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:left;"> EpiExpert </td>
   <td style="text-align:left;"> case </td>
   <td style="text-align:right;"> 39890.4889 </td>
   <td style="text-align:right;"> 5963.6015 </td>
   <td style="text-align:right;"> 15410.8183 </td>
   <td style="text-align:right;"> 18516.06907 </td>
   <td style="text-align:right;"> 0.1207500 </td>
   <td style="text-align:right;"> 55140.7628 </td>
   <td style="text-align:right;"> 0.6860274 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Hub-ensemble </td>
   <td style="text-align:left;"> case </td>
   <td style="text-align:right;"> 52275.2646 </td>
   <td style="text-align:right;"> 11842.9351 </td>
   <td style="text-align:right;"> 10888.6441 </td>
   <td style="text-align:right;"> 29543.68530 </td>
   <td style="text-align:right;"> 0.0712500 </td>
   <td style="text-align:right;"> 70436.2915 </td>
   <td style="text-align:right;"> 0.8990179 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> EpiNow2 </td>
   <td style="text-align:left;"> case </td>
   <td style="text-align:right;"> 94279.6824 </td>
   <td style="text-align:right;"> 19191.8013 </td>
   <td style="text-align:right;"> 9458.8663 </td>
   <td style="text-align:right;"> 65629.01473 </td>
   <td style="text-align:right;"> 0.0727500 </td>
   <td style="text-align:right;"> 128018.5500 </td>
   <td style="text-align:right;"> 1.6214001 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Hub-ensemble </td>
   <td style="text-align:left;"> death </td>
   <td style="text-align:right;"> 566.8165 </td>
   <td style="text-align:right;"> 197.7909 </td>
   <td style="text-align:right;"> 142.8908 </td>
   <td style="text-align:right;"> 226.13470 </td>
   <td style="text-align:right;"> -0.0545000 </td>
   <td style="text-align:right;"> 885.5352 </td>
   <td style="text-align:right;"> 0.6710764 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> EpiExpert </td>
   <td style="text-align:left;"> death </td>
   <td style="text-align:right;"> 632.4796 </td>
   <td style="text-align:right;"> 156.0871 </td>
   <td style="text-align:right;"> 242.2276 </td>
   <td style="text-align:right;"> 234.16496 </td>
   <td style="text-align:right;"> -0.0757500 </td>
   <td style="text-align:right;"> 926.6034 </td>
   <td style="text-align:right;"> 0.7571861 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> EpiNow2-convolution </td>
   <td style="text-align:left;"> death </td>
   <td style="text-align:right;"> 729.0783 </td>
   <td style="text-align:right;"> 242.6544 </td>
   <td style="text-align:right;"> 411.9277 </td>
   <td style="text-align:right;"> 74.49629 </td>
   <td style="text-align:right;"> -0.1529167 </td>
   <td style="text-align:right;"> 941.2708 </td>
   <td style="text-align:right;"> 1.1232176 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> EpiNow2 </td>
   <td style="text-align:left;"> death </td>
   <td style="text-align:right;"> 1530.7788 </td>
   <td style="text-align:right;"> 453.8853 </td>
   <td style="text-align:right;"> 129.7721 </td>
   <td style="text-align:right;"> 947.12152 </td>
   <td style="text-align:right;"> 0.2437500 </td>
   <td style="text-align:right;"> 2185.3375 </td>
   <td style="text-align:right;"> 1.7521103 </td>
  </tr>
</tbody>
</table>


![](https://i.imgur.com/OoV5aOJ.png)
