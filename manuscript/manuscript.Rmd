---
title: "Comparing human and model-based forecasts of COVID-19 in Germany and Poland"
output: 
  bookdown::pdf_document2:
    extra_dependencies: ["float"]
    keep_tex:  true
bibliography:
  - references.bib
  - GermanPolishpaper.bib
---

*Anonymous Alpaca and many friends*

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, 
                      out.width = "100%", 
                      fig.pos = "H", out.extra = "")
library(kableExtra)
library(magrittr)
library(knitr)
```

# Abstract (max 200 words, no subheadings)

Model-based forecasts, which have played an important role in shaping public policy throughout the Covid-19 pandemic, represent an implicit combination of model assumptions and the researcher’s subjective opinion. This work explicitly separates human opinion and model-derived insights to assess and compare the relative strengths and weaknesses of both approaches. We compared purely opinion-derived forecasts of cases and deaths from COVID-19 in Germany and Poland, elicited from researchers and volunteers (“crowd forecasts”), against predictions from two semi-mechanistic epidemiological models. In addition, we compared these forecasts against an ensemble of model-based, but expert-tuned, forecasts, submitted to the German and Polish Forecast Hub by other research institutions (“Hub ensemble”). We found an ensemble of crowd-sourced predictions to outperform all other methods when predicting cases (Weighted Interval Score relative to the Hub ensemble at a two-week horizon: 89%), but not when predicting deaths (rel. WIS 126%). Crowd forecasts were severely overconfident (55% and 75% coverage of the 95% prediction intervals for cases and deaths, respectively) when compared to model-based predictions. Performance of the untuned models deteriorated quickly with increasing forecast horizon when assumptions were no longer met (rel. WIS > 164% for all four week ahead predictions). 

# Introduction

Infectious disease forecasting has a long tradition and has helped inform public health decisions about (among others) influenza [CITATION], dengue fever [Citation], ebola [Citation], chikungunya and now COVID-19 [Citation]. The aggregation of forecasts of COVID-19 cases, hospitalisations or deaths from multiple teams has created opportunities to compare the predictive performance of different models (CITE e.g UK, US and German/Polish papers). Yet it has been difficult to discern which factors lead to good COVID-19 forecasts. Answering this question is complicated by the varying and generally unobserved degree of manual expert intervention. Model-based forecasts usually represent an implicit combination of human judgement and model-derived insights, in the sense that forecasts have been generated with the help of epidemiological or statistical models, but throughout the process were manually changed and adjusted according to the researchers’ judgement. 

Previous work has examined human predictions in various contexts, such as geopolitics [@tetlockForecastingTournamentsTools2014; @atanasovDistillingWisdomCrowds2016], meta-science [@hoogeveenLaypeopleCanPredict2020; ](Cite Replication Markets), and epidemiology [@farrowHumanJudgmentApproach2017; , @mcandrewExpertJudgmentModel2020, @recchiaHowWellDid2021] (Also Cite that other paper). Several prediction platforms (Metaculus 2020, Hypermind, Foretell) and prediction markets [@Predictit] have been created to collate expert and non-expert predictions. However, with the notable exception of Farrow et al., these crowd forecasts  were not designed to be evaluated alongside model-based forecasts and usually follow their own (often binary) formats. Farrow et al. found that humans could outperform computer models at predicting the 2014/15 and 2015/16 flu season in the US, a setting where the disease was well known and information about previous seasons was available. They tended to do slightly worse at predicting the 2014/15 outbreak of chikungunya in the Americas, a disease previously largely unobserved and unknown there and then. 

In this study, we attempt to further shed light on the role of human judgement and model-derived insight and analyse respective strengths and weaknesses by comparing human forecasts and model-based predictions. 

<!-- In contrast to Farrow et al., we analyse not only point predictions, but also for their predictive uncertainty, to exploit the greater utility of probabilistic forecasts (CITE Held, Meyer, Bracher, 2017).  -->

# Methods 

We created a crowd-sourced forecast (“crowd forecast”), elicited from participants through a web application [Citation], as well as two semi-mechanistic models ("renewal model" and "convolution model") informed by basic assumptions about COVID-19 epidemiology. Forecasts were created in real time over a period of 21 weeks from October 12th 2020 until March 1st 2021 and submitted to the German and Polish Forecast hub (CITATION). All code and tools necessary to generate the forecasts and make a forecast submission are available in the `covid.german.forecasts` R package (N. Bosse et al. 2020). This repository also contains a record of all forecasts submitted to the German and Polish Forecast Hub. Forecasts were evaluated using a variety of scoring metrics and compared among each other and against an ensemble of all other models submitted to the Forecast Hub. 

## Forecast targets and interaction with the German and Polish Forecast Hub

The German and Polish Forecast Hub elicits predictions for various COVID-19 related forecast targets from different research institutions every week. Forecasts had to be made every Monday (with submissions allowed until Tuesday 3pm) using data available until Monday 11.59pm. We submitted forecasts for incident and cumulative weekly reported numbers of cases and deaths from COVID-19 on a national level in Germany and Poland over a one to four week forecast horizon. Weeks were defined as epiweeks starting on Sunday and ending on Saturday, meaning that forecast horizons were in fact 5, 12, 19 and 26 days. Submissions were required in a quantile-based format with 23 quantiles at levels 0.01, 0.025, 0.05, 0.10, 0.15, ..., 0.95, 0.975, 0.99. Forecasts submitted to the Forecast Hub were combined to different ensembles every week, with the median ensemble being the default ensemble shown on all official Forecast hub visualisations. 

Data on daily reported test positive cases and deaths linked to Covid-19 were provided by the organisers of the German and Polish forecast hub (J. Bracher et al. 2021). Until December 14th 2020 these data were sourced from the European Centre for Disease Control (ECDC) (“Download Historical Data (to 14 December 2020) on the Daily Number of New Reported COVID-19 Cases and Deaths Worldwide” 2020). After ECDC stopped publishing daily data, observations were sourced from the Robert Koch Institute (RKI) and the Polish Ministry of Health for the remainder of the submission period (“RKI - Coronavirus SARS-CoV-2 - Aktueller Lage-/Situationsbericht Des RKI Zu COVID-19” n.d.). These data are subject to reporting artefacts (such as a delayed case reporting in Poland on the 24th November (“Rozbieżności w statystykach koronawirusa. 22 tys. przypadków będą doliczone do ogólnej liczby wyników” 16:07:56+0100)), changes in reporting over time and variation in testing regimes (e.g. in Germany from the 11th of November on (Ärzteblatt 2020)).

## Crowd forecasts

Our crowd forecasts were created as an ensemble of forecasts made by individual participants every week through a web application  (https://cmmid-lshtm.shinyapps.io/crowd-forecast/). Forecasts were allowed until 12pm on Tuesdays, but participants were asked to only use information available until Monday. The application was built using the shiny and golem R packages (Chang et al. 2021; Fay et al. 2021) and is available in the crowdforecastr R package (N. I. Bosse et al. 2020). To make a forecast in the application participants could select a predictive distribution, with the default being log-normal, and adjust the median and the width of the uncertainty by either interacting with a figure showing their forecast or providing numerical values. The baseline shown was a repetition of the last known observation with constant uncertainty around it computed as the standard deviation of the last four changes in log observed forecasts (i.e. as `sd(c(log(value4) - log(value3), log(value3) - log(value2), ...))`). 
Our interface also allowed participants to view the observed data, and their forecasts, on a logarithmic scale and presented additional contextual COVID-19 data sourced from (“COVID-19 Data Explorer” n.d.). These data included e.g. notifications of both test positive COVID-19 cases and COVID-19 linked deaths and the number of COVID-19 tests conducted over time. 

Forecasts were stored in a Google Sheet and downloaded, cleaned and processed every week for submission. If a forecaster had submitted multiple predictions for a single target, only the latest submission was kept. Information on the chosen distribution as well as the parameters for median and width were used to obtain the required set of 23 quantiles from that distribution. Forecasts from all forecasters were then aggregated using an unweighted quantile-wise mean. On a few occasions, individual forecasts were assessed as clearly erroneous by visual inspection and subsequently removed before aggregation. 

Participants were recruited mostly within the Centre of Mathematical Modeling of Infectious Diseases at the London School of Hygiene & Tropical Medicine, but participants were also invited personally or via social media to submit predictions. Depending on whether they had a background in either statistics, forecasting or epidemiology, participants were asked to self-identify as 'experts' or 'non-experts'. 

## Model-based forecasts

We used two models from the `EpiNow2` R package (version 1.3.3) as our model-based forecasts (Abbott, Hellewell, et al. 2020). The first of these models, here called “renewal model”, used the renewal equation [@fraserEstimatingIndividualHousehold2007] to predict reported cases and deaths. It estimated the effective reproduction number $R_t$ (the average number of people each infected person is expected to infect in turn) and models future infections as a weighted sum of past infections times $R_t$. $R_t$ was assumed to stay constant in the future, roughly corresponding to continuing the latest trend in infections. The $R_t$ value which was carried forward was slightly changed on November 9th 2020, where before that date the last 'reliable' $R_t$ estimate from X days before the forecast was used and after that the latest and most up-to-date (but possibly less stable) $R_t$ estimate made on the actual forecast date was used. Reported cases and deaths were obtained by convolving predicted infections over data-based delay distributions (Abbott, Hellewell, et al. 2020; DOI n.d.; “Evaluating the Use of the Reproduction Number as an Epidemiological Tool, Using Spatio-Temporal Trends of the Covid-19 Outbreak in England | medRxiv” n.d.) to model the time between infection and report date or death. The renewal model was used to predict cases as well as deaths, but the two were forecast completely independently, meaning that $R_t$ was forecast once from reported cases and once from reported deaths. Death forecasts from the renewal model were therefore not informed by past cases. One submission of the renewal model on December 28th 2020 was belated and therefore not included in the official Forecast hub ensemble. 

The second model (“convolution model”) was only used to forecast deaths and was added later, starting December 7th 2020 (with the first forecast from December 7th suffering from a software bug and therefore disregarded in all further analyses). The convolution model was submitted, but never included in the official Forecast hub ensemble due to concerns that it could be too similar to the renewal model. The convolution model predicted deaths these as a fraction of infected people who would die with some delay, by using a convolution of reported cases with a distribution that described the delay from case report to death and a scaling factor (the case-fatality ratio). Both the renewal and the convolution model used daily observations and assumed a negative binomial observation model and a multiplicative day-of-the-week effect (Abbott, Hellewell, et al. 2020). Line list data used to inform the delay from symptom onset to test positive case report or death in the model-based forecasts was sourced from (Xu et al., n.d.) with data available up to the 1st of August. All model fitting was done using Markov-chain Monte Carlo (MCMC) in stan (Stan Development Team 2020) with each location and forecast target being fit separately. More details are available in the supplementary information. 

## Analysis

For the main analysis we focused on two week ahead predictions, as predictions beyond this horizon are often unreliable due to rapidly changing conditions (J. Bracher et al. 2021). Forecasts for cases were scored using the full period from October 2020 until March 2021. To ensure comparability between models, all death forecasts were scored using only the period from December 14th on, where all models including the convolution model were available. To ensure robustness of our results we conducted a sensitivity analysis where all forecasts (including cases) where scored only over the later period where are forecasts were available (see Section \@ref(sensitivity-analysis) in the SI). Results remained broadly unchanged. 

Forecasts were analysed using the following scoring metrics: The weighted interval score (WIS) (Johannes Bracher et al. 2021), the absolute error, relative bias, and empirical coverage of the 50% and 90% prediction intervals. The WIS is a proper scoring rule (CITE GNEITING) and can be understood as a generalisation of the absolute error to quantile-based forecasts. It can be decomposed into three separate penalties: forecast spread (i.e. uncertainty of forecasts), over-prediction and under-prediction - lower values are always better. While the over- and under-prediction components of the WIS capture the amount of over-prediction and under-prediction in absolute terms, we also look at a relative tendency to make biased forecasts. The bias metric we use captures how much probability mass of the forecast was above or below the true value and therefore represents a general tendency to over- or under-predict in relative terms. It is bound between -1 (all quantiles of the predictive distribution are below the observed value) and 1 (all quantiles are above the observed value). Empirical coverage is the percentage of observed values that fall inside a given prediction interval (e.g. how many observed values fall inside all 50% prediction intervals). Scoring metrics are explained in more detail in Table \@ref(tab:scoring-metrics) in the SI. All scores were calculated using the `scoringutils` R package (N. Bosse 2020). 

At all stages of the evaluation our forecasts were compared to the median ensemble of all other models submitted to the German and Polish Forecast Hub ("Hub ensemble"). This "Hub ensemble" was retrospectively computed and excludes all our models. It therefore differs from the official Hub ensemble (here called "hub-ensemble-realised") which included crowd forecasts as well as renewal model forecasts. 

The ensemble that we call "Hub ensemble" in this paper differs from the 


in that we removed our forecasts to allow for a more straightforward comparison. To enhance interpretability of scores we mainly report WIS relative to the Hub ensemble in the main text, i.e. we divided all scores by those achieved by the Hub ensemble on the same set of forecasts. In addition to comparing our forecasts against the hub ensemble excluding our models, we also assessed the impact of our forecasts on the performance of the forecasting hub by recalculating separate versions of the Hub ensemble with only some (or all) of our forecasts included. 

Versions that included either all of our models (“hub-ensemble-with-all”) or only one of them (“hub-ensemble-with-X”) were computed retrospectively. 





# Results

## Crowd forecast participation

A total number of 32 participants submitted forecasts, 17 of those self-identified as 'expert'. The median number of forecasters for any given forecast target was 6, the minimum 2 and the maximum 10. The mean number of submissions from an individual forecaster was 4.7 but the median number was only one - most participants dropped out after their first submission. Only two participants submitted a forecast every single week, both of whom are authors on this study.

```{r}
caption <- "Visualisation of aggregate performance metrics across forecast horizons. A, B: mean weighted interval score (WIS, lower indicates better performance) across horizons. WIS is decomposed into its components dispersion, over-prediction and under-prediction. C: Empirical coverage of the 50\\% prediction intervals (50\\% coverage is perfect). D: Empirical coverage of the 90\\% prediction intervals. E: Dispersion (same as in panel A, B). Higher values mean greater dispersion of the forecast and imply ceteris paribus a worse score. F: Bias, i.e. general (relative) tendency to over- or underpredict. Values are between -1 (complete under-prediction) and 1 (complete over-prediction) and 0 ideally. G: Absolute error of the median forecast (lower is better). H. Standard deviation of all WIS values for different horizons"
```

```{r agg-performance-all, fig.cap=caption}
knitr::include_graphics("../analysis/plots/aggregate-performance-all-v4.png")
```



```{r}
caption <- "A, C: Visualisation of 50\\% prediction intervals of two week ahead forecasts against the true observed values. Forecasts that were not scored (because there was no complete set of death forecasts available) are greyed out. B, D: Visualisation of corresponding WIS."
```

```{r forecasts-and-truth, fig.cap=caption}
knitr::include_graphics("../analysis/plots/figure-forecasts-2.png")
```

## Case Forecasts

For cases, crowd forecasts had a lower mean weighted interval score (WIS, lower values indicate better performance) than both the renewal model and the Hub ensemble across all forecast horizons (Figure \@ref(fig:agg-performance-all)A) and locations (Figure \@ref(fig:performance-locations-rel)A). For two week ahead forecasts, mean WIS relative to the Hub ensemble (= 100%) was 89% for crowd forecasts and 140% for the renewal model (Table \@ref(tab:score-table-2)). Across all forecasting approaches, locations and forecast horizons, the distribution of WIS values was right-skewed, and average performance was heavily influenced by outliers (Figure \@ref(fig:distribution-scores)). Overall, low variance in forecast performance was closely linked with good mean performance (Figures \@ref(fig:agg-performance-all)H and and \@ref(fig:agg-performance-all)A), suggesting that the ability to avoid large errors was an important factor in determining overall performance. The impact of outlier values was especially pronounced for the renewal model, which had more outliers (Figure \@ref(fig:distribution-scores), as well as the highest standard deviation of WIS values (relative WIS sd 1.54 at the two weeks ahead horizon), while the ensemble of crowd forecasts  (rel. WIS sd 0.76) and the Hub ensemble (= 1) showed more stable performance. 

To varying degrees, all models exhibited trend-following behaviour and were rarely able to predict a change in trend before it had happened. For example, all models failed to predict the change in trend from increase to decrease that happened in early November... in Germany and severely overshot (Figure \@ref(fig:forecasts-and-truth)A). This was most striking for the renewal model, which extrapolated unconstrained exponential growth based on the recent past  of observations. The Hub ensemble and the crowd forecast, which had both been under-predicting throughout October, also failed to predict the change in trend after cases peaked, but less severely so. Human forecasters, possibly aware of the semi-lockdown announced on November 2nd 2020 (Deutsche Welle (www.dw.com) n.d.) and the change in the testing regime (with stricter test criteria) on November 11th 2020 (Ärzteblatt 2020), were fastest to adapt to the new trend, and the Hub ensemble slowest. In December, cases rose again in Germany, with all models under-predicting this growth to varying extents. As in October, the renewal model captured the phase of exponential growth in cases slightly better than other approaches, but again overshot when reported case numbers fell over Christmas. The large variance in predictions in January in Germany (severe under-prediction followed by severe over-prediction) may in part be caused by the fact that the renewal model operated on daily data and therefore was susceptible to fluctuations in daily reporting around Christmas that would not have influenced on weekly reporting. Similar trends in performance were evident in Poland, with the crowd forecast quickest at adapting to the change in trend in November. In general, there were fewer large outlier forecasts in Poland and in particular the renewal model performed more in line with other forecasts there. 

All forecasting approaches, including the Hub ensemble, were overconfident and showed lower than nominal coverage (meaning that 50% (90%) prediction intervals generally covered less than 50% (90%) of the actually observed values) (Figure \@ref(fig:agg-performance-all)C and \@ref(fig:agg-performance-all)D). Coverage for all forecasts deteriorated with increasing forecast horizon, indicating that all forecasting approaches struggled to quantify uncertainty appropriately for case forecasts. This was especially an issue for crowd forecasts, which were markedly sharper (i.e., narrower) than other approaches (Figure \@ref(fig:agg-performance-all)E) and only showed a small increase in uncertainty across forecast horizons. In spite of good performance in terms of the absolute error (Figure \@ref(fig:agg-performance-all)G), excessive sharpness led to forecasts which were severely overconfident (covering only 36% and 55% of observations with their 50% and 90% prediction intervals at a two week horizon, and only 5% and 38% four weeks ahead) (Figure \@ref(fig:agg-performance-all)C,D and Tables \@ref(tab:score-table-2) and \@ref(tab:score-table-4)). Despite worse performance in terms of absolute error (Figure \@ref(fig:agg-performance-all)G), the renewal model achieved better calibration (comparable to the Hub ensemble), as uncertainty increased rapidly and non-linearly across forecast horizons.   

The renewal model exhibited a noticeable tendency towards over-predicting reported cases across all horizons. The crowd forecast tended to over-predict at longer forecast horizons, whereas the Hub ensemble showed no systematic bias (Figure \@ref(fig:agg-performance-all)F). Regardless of a general relative tendency to over-predict, all forecasting approaches incurred larger absolute penalties from over- than from under-prediction (see decomposition of the WIS into absolute penalties for over-prediction, under-prediction and dispersion in Figures \@ref(fig:agg-performance-all)A and \@ref(fig:agg-performance-all)B and Tables \@ref(tab:score-table-2) and \@ref(tab:score-table-4)), implying that over-prediction (e.g. when missing a peak) tended to be more costly in terms of the WIS. 

Generally, trends in overall performance were broadly similar across locations (Figures \@ref(fig:performance-locations) and \@ref(fig:performance-locations-rel)).
Due to the differing population sizes and numbers of notifications in Germany and Poland absolute scores were difficult to compare directly. However, relative to the Hub ensemble, the crowd forecasts performed noticeably better in Germany than in Poland and the renewal model better in Poland than in Germany (Figure \@ref(fig:performance-locations-rel)A and \@ref(fig:performance-locations-rel)G). 

## Death Forecasts

For deaths, the Hub ensemble outperformed the crowd forecasts as well as our model-based approaches across all forecast horizons and locations (Figure \@ref(fig:agg-performance-all)B, Figure \@ref(fig:performance-locations)B). Relative WIS values for the models two weeks ahead were 1.22 (convolution model), 1.26 (crowd forecast), 1 (Hub ensemble) and 1.79 (renewal model). The crowd forecasts performed better than the renewal model across all forecast horizons and locations (Figure \@ref(fig:agg-performance-all)B, Figure \@ref(fig:performance-locations)B), and also better than the convolution model three and four weeks ahead. Poor performance of the renewal model, especially at longer horizons, indicates that an approach that does not know about past cases, but instead estimates and projects a separate $R_t$ trace from deaths, is not well suited for the task. The convolution model was able to outperform both the renewal model and the crowd forecasts at shorter forecast horizons (where the delay between cases and deaths means that future deaths are largely informed by present cases), but saw performance deteriorate at three and four weeks ahead (where case predictions from the renewal model were increasingly used to inform death predictions) (Figure \@ref(fig:agg-performance-all)B, Table \@ref(tab:score-table-4)). 

Theoretical considerations suggest that predicting a change in trend may be easier for deaths than for cases (as past cases, hospitalisations and other data can be used as predictors). Even though all forecasts generally struggled with this, there were some instances where changing trends were well captured or even anticipated. In Poland, for example, the Hub ensemble was able to capture or even anticipate the peak in deaths in December quite well (whereas the renewal model and crowd forecast did not). The renewal model, which mostly exhibited trend-following behaviour, correctly predicted another increase in weekly deaths in mid-January (potentially based on changes in daily deaths, as the renewal model did not know about past cases). In Germany in early January, all models predicted a decrease in deaths two to three weeks before it actually happened. Predictions from the renewal model at that time were likely strongly influenced by an unexpected drop in reported deaths on December XXth. The other forecasting approaches and in particular, the convolution model may have been affected by potentially under-reported case numbers around Christmas. When the decrease that all models had predicted to happen in early January failed to materialise, the renewal model and the crowd forecast noticeably over-corrected and over-predicted deaths in the following weeks, while the Hub ensemble, and to a slightly lesser degree, the convolution model were able to capture the downturn well when it finally happened at the end of January. 

Death forecasts, generally, showed greater coverage of the 50% and 90% prediction intervals than case forecasts and no decrease in coverage across forecast horizons, indicating that it might be easier to appropriately quantify uncertainty for death forecasts. The Hub ensemble had the greatest coverage, with empirical coverage of the 50% and 90% prediction intervals exceeding 50%, and 90%, respectively, across all forecast horizons. Coverage for the crowd forecasts and our model-based approaches was generally lower than that of the Hub ensemble and mostly slightly lower than nominal coverage (Figure \@ref(fig:agg-performance-all)C and \@ref(fig:agg-performance-all)D). As for cases, the crowd forecast tended to be the sharpest and uncertainty increased most slowly across forecast horizons, and the renewal model forecasts generally were widest. The convolution model was relatively sharp for short forecast horizons, but had rapidly (and non-linearly) increasing uncertainty for longer forecast horizons, driven by increasing uncertainty in the underlying case forecasts. 

For deaths, the ensemble of crowd forecasts had a consistent tendency to over-predict \@ref(fig:agg-performance-all)F. The convolution model had a strong tendency to under-predict, which steadily decreased for longer forecast horizons. The renewal model (which over-predicted for cases) and the Hub ensemble slightly tended towards under-prediction. For deaths, absolute over- and under-prediction penalties were more in line with a general relative tendency to over- or under-predict than for cases (Figure \@ref(fig:agg-performance-all)A, \@ref(fig:agg-performance-all)B and Tables \@ref(tab:score-table-2), \@ref(tab:score-table-4)). 

```{r}
caption <- "A: Distribution of weighted interval scores for two week ahead forecasts of the different models and forecast targets. B: Distribution of WIS separate by country."
```

```{r distribution-scores, fig.cap=caption}
knitr::include_graphics("../analysis/plots/distribution_scores_wis-2.png")
```


```{r}
caption <- "Visualisation of aggregate performance metrics across forecast horizons for the different versions of the Hub median ensemble. “Hub-ensemble” excludes all our models, Hub-ensemble-all includes all of our models, “Hub-ensemble-real” is the real hub-ensemble with the renewal model and the crowd forecasts included. Values (except for Bias) are computed as differences to the Hub ensemble excluding our contributions. For Coverage, this is an absolute difference, for other metrics this is a percentage difference. A: mean weighted interval score (WIS) across horizons. B: median WIS. C: Absolute error of the median forecast. D: Standard deviation of the WIS. E: Dispersion (higher values mean greater spread of the forecast). F: Bias, i.e. general tendency to over- or underpredict. Values are between -1 (complete under-prediction) and 1 (complete over-prediction) and 0 ideally. G: Empirical coverage of the 50\\% prediction intervals. F: Empirical coverage of the 90\\% prediction intervals"
```

```{r agg-performance-ensemble, fig.cap=caption}
knitr::include_graphics("../analysis/plots/aggregate-performance-rel-ensemble-v4.png")
```

## Contribution to the Forecast Hub

Of our three models, only the renewal model and the crowd forecast were included in the official Forecast Hub median ensemble (“hub-ensemble-realised”), while the convolution model was never included as it was deemed too similar to the existing renewal model, thus potentially introducing bias. In the official Hub ensemble, there were on average 7.1 models included (including our own), with a median of 7, a minimum of 4 (on December 28 2020 over the Christmas period) and a maximum of 10. Versions that included either all of our models (“hub-ensemble-with-all”) or only one of them (“hub-ensemble-with-X”) were computed retrospectively. An overview of all models and ensemble versions is shown in Table \@ref(tab:table-ensemble-versions) in the SI. 

For cases, our contributions (compared to the Hub ensemble without our contributions) consistently improved performance across all forecasting horizons (rel. WIS 0.9 two weeks ahead, Table \@ref(tab:score-table-ensemble-2)). Contributions from the crowd forecasts alone also were positive across all forecast horizons, while contributions from the renewal model became negative for longer horizons (rel. WIS 1.02 three weeks ahead, 1.06 four weeks ahead). The realised ensemble including both models performed better or equal compared to all versions with only one model included for up to three weeks ahead, suggesting synergistic effects. Only for four week ahead predictions would removing the renewal model have improved performance (Table \@ref(tab:score-table-ensemble-4)). 

For deaths, contributions from the renewal model and crowd forecast together improved performance only for one week ahead predictions and showed an increasingly negative impact on performance for longer horizons (rel. WIS of the hub-ensemble-realised 1.01 two weeks ahead, 1.05 four weeks ahead, Tables \@ref(tab:score-table-ensemble-2) and \@ref(tab:score-table-ensemble-4)). Individual contributions from both the renewal model and the crowd forecast were largely negative, while a version of the Hub ensemble with only the convolution model included would have performed consistently better across all forecast horizons (with the positive impact increasing for longer horizons). This is especially interesting as the convolution model performed consistently worse than the pre-existing Hub ensemble (Figure \@ref(fig:agg-performance-all)) and especially worse for longer horizons. 

We also considered the impact of our contributions on the mean, rather than the median ensemble. 
<!-- In general, the median ensemble tended to perform slightly better than the mean ensemble - needs more analysis -->
General trends were similar, with the notable exception of the convolution model, which had a consistently positive impact on the median ensemble, but a mixed and mostly slightly negative impact on the mean ensemble (Figures \@ref(fig:agg-performance-ensemble)B and \@ref(fig:agg-performance-ensemble-mean)B). This may happen if a model is more correct directionally relative to the pre-existing ensemble, but overshoots in absolute terms, thereby moving the ensemble too far. For both the mean and the median ensemble, changes in performance from adding or removing models were of a similar order of magnitude, suggesting that at least in this instance, with a relatively small ensemble size, the median ensemble was not necessarily more ‘robust’ to changes than the mean ensemble. However, the ensemble version with all our forecasts included (“hub-ensemble-with-all”) tended to perform relatively better for the median ensemble than the mean ensemble, suggesting that adding more models may be more beneficial or ‘safer’ for the median than for the mean ensemble as directional errors can more easily cancel out than errors in absolute terms. 

# Discussion
Epidemiological forecasting usually represents a mix between human insight and model-based assumptions. In this study, we compared purely opinion-derived forecasts of cases and deaths from COVID-19 in Germany and Poland, elicited from a crowd of researchers and volunteers against predictions from two semi-mechanistic epidemiological models. In spite of the small number of participants and a general tendency to be overconfident, crowd forecasts consistently outperformed our epidemiological models as well as the Hub ensemble when forecasting cases but not when forecasting deaths. This suggests that humans are relatively good at foreseeing trends that are hard to model but may struggle to form an intuition for the exact relationship between cases and deaths. 

Past studies have evaluated the performance of model-based forecasting approaches as well as human experts and non-experts in various contexts. However, most of these studies either focused only on the evaluation of (expert-tuned) model-based approaches (Cite US Hub, cite Seb SPI-M), or exclusively on human forecasts (e.g. cite Tetlock, cite Grecchia, cite McAndrew). In contrast, we directly compared human and model-based forecasts. This is similar to the approach taken by Farrow et al., but extends it in several ways. While Farrow et al. only asked for point predictions and constructed a probabilistic forecast from these, we asked participants to quantify their uncertainty directly, allowing us to compare human forecasts and models without any further assumptions, as well as to analyse aspects of the forecasts like overconfidence. In addition, we compared crowd forecasts to two semi-mechanistic models informed by basic epidemiological knowledge of COVID-19, allowing us to assess not only relative performance but also to analyse qualitative differences between human judgement and model-derived insight. In terms of interpretability of the results, exact knowledge of our two models, as well as focus on a limited set of targets and locations was a major advantage of our study compared to larger studies conducted by the Forecast Hubs (J. Bracher et al. 2021; E. Y. Cramer et al. 2021; S. Funk et al. 2020). 

The good performance of crowd forecasts we found is in line with results from Farrow et al. who also report strong performance of human predictions in past Flu challenges despite difficulties to recruit a large number of participants. The advantage of crowd forecasts we observed over our semi-mechanistic models is likely in part explained by the fact that we compared an ensemble of crowd forecasts with single models. However, this probably explains only part of the difference and performance relative to the Hub ensemble strongly suggests that human insight is valuable when predicting highly volatile and hard-to-predict quantities such as case numbers. Relatively good performance of our semi-mechanistic models short-term, but not longer-term, suggests that model-based forecasts are helpful to extrapolate from current conditions, but require some form of human intervention or additional assumptions to inform forecasts when conditions change over time. This human intervention may be particularly important when dealing with artefacts in reporting and data anomalies (and especially when using daily, rather than weekly data). The large variance in predictions in January in Germany for example (severe under-prediction followed by severe over-prediction, see Figure \@ref(fig:forecasts-and-truth)A), may in part be caused by the fact that the renewal model operated on daily data and therefore was susceptible to fluctuations in daily reporting around Christmas that would not have influenced on weekly reporting. While we were not able to observe how expert opinion informed other models submitted to the Forecast Hub, conversations with the Hub organisers suggest that forecasts tended to benefit from a large degree of human intervention. Unfortunately, we were not able to test a scenario in which users were asked to alter an existing model output. We can therefore make no conclusive statements about the direct effects of researchers tuning their models, rather than create a new forecast from a naive baseline. 

Conversely, our results suggest that human intervention may be less beneficial when forecasting deaths (especially at shorter horizons, when deaths are largely dependent on already observed cases), which benefits from the ability to model the delays and exact epidemiological relationships between different leading and lagged indicators. Relatively good performance of the convolution model, especially compared to the poor performance of the renewal model on deaths (which used only deaths to estimate and predict the effective reproduction number) underlines the importance of including leading indicators such as cases as a predictor for deaths. 

Given the low number of participants in our study, it is difficult to generalise conclusions about crowd predictions to other settings. In particular, our crowd forecasting application was not developed to a high standard of user friendliness. This may have precluded interested parties from submitting forecasts. The low number of participants presumably resulted in higher variance of forecasts, but also potentially in better average forecast performance. Motivating forecasters to contribute regularly proved challenging, especially given that the majority of our participants were from the UK and had little connection to Germany and even less to Poland. In addition, lack of effective outreach played an important role, as did a lack of time and resources to design the interface in a way that is appealing enough to attract large audiences outside of academia. Using an R shiny app as an interface arguably created some limits to user experience and performance, influencing the number of participants. On the other hand, it facilitated quick development and allows us to provide our crowd forecasting tooling as an open source R package, meaning that it is available for others to use, and to further develop, in their context. 

In general, our results support the use of ensembles, rather than individual models to inform public health policies, as these provide consistently good performance and largely avoid particularly bad outlier forecasts. Our results also suggest that at least for a small median ensemble like the German and Polish Forecast Hub ensemble adding more models may increase performance in most instances (with the version of the hub ensemble that includes all our contributions ("hub-ensemble-with-all") outperforming the realised official hub ensemble in almost all instances). We often found contributions to an ensemble to be beneficial even in instances where the forecasting approach entering the ensemble performed consistently worse than the pre-existing ensemble. Including more models may be easier when using a median ensemble, as extreme forecasts tend to have less of an influence there. Individual models, however, can still have a place in informing public policy. As an average, ensembles show the consensus opinion, which implies that in contrast to single models, they are not likely to reveal information, e.g. about future changes in trend, that is not in some way ‘obvious’ to the majority of forecasters. 

In the context of evaluating different models and the merits of human intervention, what constitutes a ‘good’ forecast very much depends on the purpose of that forecast. Policymakers may care much more about certain types of errors, (e.g. under-prediction) than others in a way not captured by the weighted interval score. Also, it is helpful to distinguish between ‘forecasts’ and ‘projections’. For example, the outperformance of the crowd forecasts on cases and in general on longer forecast horizons is perhaps not surprising, given that humans could factor in an expectation of future changes in conditions (e.g. future policy interventions, adherence to these interventions, testing policy, and the evolution of new disease variants) while our model-based forecasts could not. However, if the aim of a forecast is to inform policy and decision making, then attempting to factor in the interventions one is meant to inform may be problematic (and conversely, arguing for or against interventions based on predictions may be difficult depending on what the forecast assumes about future interventions). In that sense, crowd forecasts can be understood as forecasts, whereas our model predictions may be better understood as projections that show what would happen in the absence of any events that could change the trend. This may pose a problem in the context of a forecast hub if teams have a different understanding about the purpose of a forecast. Projections that are meant to inform policy makers about possible scenarios may be different from forecasts meant to inform the general public, and yet again different from forecasts submitted with the goal of advancing open research and in order to allow public scrutiny and discussion about the merits of different approaches.   
Further work should explore the effects of human intervention on model-based forecasts in more detail. Model-based forecasts could be used as an input to human judgement, with researchers adjusting predictions generated by models. Seeing a model-based forecasts could help humans calibrate uncertainty better, while allowing for manual intervention to adapt spurious trend predictions. Tools need to be developed to facilitate this process at a larger scale. Human insight could also be used as an input to models. Such a ‘hybrid’ forecasting approach could for example ask humans to predict the trend of the effective reproduction number $R_t$ or the doubling rate (i.e. how the epidemic evolves) into the future and use this to estimate the exact number of cases, hospitalisations or deaths this would imply. This approach seems especially promising for forecasting deaths if a model can accurately describe delay distributions and epidemiological relationships between different parameters. In light of severe overconfidence, yet good performance in terms of the absolute error, post-processing of human forecasts to adjust and widen confidence intervals may be another promising approach. Crowd forecasting in general could benefit greatly from the availability of tools suitable to appeal to a greater audience. Given the good performance we and previous authors observed in spite of the limited resources available and the small number of participants, this seems worthwhile for researchers and policymakers to further develop and explore. 












# (APPENDIX) Supplementary information {-} 

# Supplementary information

## Scoring metrics used

```{r scoring-metrics}
wis <- list(
  Metric = "WIS (Weighted) interval score", 
  `Explanation` = r"(The weighted interval score (smaller values are better) is a proper scoring rule for quantile forecasts. It converges to the continuos ranked probability score (which itself is a generalisation of the absolute error to probabilistic forecasts) for an increasing number of intervals. The score can be decomposed into a dispersion (uncertainty) component and penalties for over- and underprediction. For a single interval, the score is computed as 
  $$IS_\alpha(F,y) = (u-l) + \frac{2}{\alpha} \cdot (l-y) \cdot 1(y \leq l) + \frac{2}{\alpha} \cdot (y-u) \cdot 1(y \geq u), $$ 
  where $1()$ is the indicator function, $y$ is the true value, and $l$ and $u$ are the $\frac{\alpha}{2}$ and $1 - \frac{\alpha}{2}$ quantiles of the predictive distribution $F$, i.e. the lower and upper bound of a single prediction interval. For a set of $K$ prediction intervals and the median $m$, the score is computed as a weighted sum, 
  $$WIS = \frac{1}{K + 0.5} \cdot (w_0 \cdot |y - m| + \sum_{k = 1}^{K} w_k \cdot IS_{\alpha}(F, y)),$$ 
  where $w_k$ is a weight for every interval. Usually, $w_k = \frac{\alpha_k}{2}$ and $w_0 = 0.5$. 

Its proximity to the absolute error means that when averaging across multiple targets (e.g. different weeks), it will be dominated by targets with higher absolute values.)", 
  `Caveats` = "The wis is based on measures of absolute error. When averaging across multiple targets, it will therefore be dominated by targets with higher absolute values.",
  `References` = ""
)

interval_coverage <- list(
  `Metric` = "Interval coverage", 
  `Explanation` = r"(Interval coverage is a measure of marginal calibration and indicates the proportion of observed values that fall in a given prediction interval range. Nominal coverage represents the percentage of observed values that should ideally be covered (e.g. we would like a 50 percent prediction interval to cover on average 50 percent of the observations), while empirical coverage is the actual percentage of observations covered by a certain prediction interval.)",
  `Caveats` = "",
  `References` = ""
)

bias <- list(
  `Metric` = "Bias", 
  `Explanation` = r"((Relative) bias is a measure of the general tendency of a forecaster to over- or underpredict. Values are between -1 and 1 and 0 ideally. For continuous forecasts, bias is given as 
$$B(F, y) = 1 - 2 \cdot (F (y)), $$ 
where $F$ is the CDF of the predictive distribution and $y$ is the observed value. 

For quantile forecasts, $F(y)$ is replaced by a quantile rank. The appropriate quantile rank is determined by whether the median forecast is below  or above the true value. We then take the innermost quantile rank for which the quantile is still larger (under-prediction) or smaller (over-prediction) than the observed value. 

In contrast to the over- and underprediction penalties of the interval score it is bound between 0 and 1 and represents a general tendency of forecasts to be biased rather than the absolute amount of over- and underprediction. It is therefore a more robust measurement.)",
  `Caveats` = "",
  `References` = ""
)

data <- rbind(as.data.frame(wis), 
              as.data.frame(interval_coverage),
              as.data.frame(bias))

data[, 1:2] %>%
  kableExtra::kbl(format = "latex", booktabs = TRUE, 
                  escape = FALSE,
                  caption = "Overview of the scoring metrics used.",
                  longtable = TRUE,
                  linesep = c('\\addlinespace \\addlinespace')) %>%
  kableExtra::column_spec(1, width = "2.5cm") %>%
  kableExtra::column_spec(2, width = "13.0cm") %>%
  kableExtra::kable_styling(latex_options = c("striped", "repeat_header")) 

```


\clearpage

## Further details on the semi-mechanistic forecasting models

### Renewal equation model

The model was initialised prior to the first observed data point by assuming constant exponential growth for the mean of assumed delays from infection to case report.

\begin{align}
  I_{t} &= I_0 \exp  \left(r t \right)  \\
  I_0 &\sim \mathcal{LN}(\log I_{obs}, 0.2) \\
  r &\sim \mathcal{LN}(r_{obs}, 0.2) 
\end{align}

Where $I_{obs}$ and $r_{obs}$ are estimated form the first week of observed data. For the time window of the observed data infections were then modelled by weighting previous infections by the generation time and scaling by the instantaneous reproduction number. These infections were then convolved to cases by date ($O_t$) and cases by date of report ($D_t$) using log-normal delay distributions. This model can be defined mathematically as follows,

\begin{align}
  \log R_{t} &= \log R_{t-1} + \mathrm{GP}_t \\
  I_t &= R_t \sum_{\tau = 1}^{15} w(\tau | \mu_{w}, \sigma_{w}) I_{t - \tau} \\
  O_t &= \sum_{\tau = 0}^{15} \xi_{O}(\tau | \mu_{\xi_{O}}, \sigma_{\xi_{O}}) I_{t-\tau} \\
  D_t &= \alpha \sum_{\tau = 0}^{15} \xi_{D}(\tau | \mu_{\xi_{D}}, \sigma_{\xi_{D}}) O_{t-\tau} \\ 
  C_t &\sim \mathrm{NB}\left(\omega_{(t \mod 7)}D_t, \phi\right)
\end{align}

Where,
\begin{align}
     w &\sim \mathcal{G}(\mu_{w}, \sigma_{w}) \\
    \xi_{O} &\sim \mathcal{LN}(\mu_{\xi_{O}}, \sigma_{\xi_{O}}) \\
    \xi_{D} &\sim \mathcal{LN}(\mu_{\xi_{D}}, \sigma_{\xi_{D}}) 
\end{align}

This model used the following priors for cases,

\begin{align}
     R_0 &\sim \mathcal{LN}(0.079, 0.18) \\
    \mu_w &\sim \mathcal{N}(3.6, 0.7) \\
    \sigma_w &\sim \mathcal{N}(3.1, 0.8) \\
    \mu_{\xi_{O}} &\sim \mathcal{N}(1.62, 0.064) \\
    \sigma_{\xi_{O}} &\sim \mathcal{N}(0.418, 0.069) \\
    \mu_{\xi_{D}} &\sim \mathcal{N}(0.614, 0.066) \\
    \sigma_{\xi_{D}} &\sim \mathcal{N}(1.51, 0.048) \\
    \alpha &\sim \mathcal{N}(0.25, 0.05) \\
    \frac{\omega}{7} &\sim \mathrm{Dirichlet}(1, 1, 1, 1, 1, 1, 1) \\
    \phi &\sim \frac{1}{\sqrt{\mathcal{N}(0, 1)}}
\end{align}

and updated the reporting process as follows when forecasting deaths,

\begin{align}
    \mu_{\xi_{D}} &\sim \mathcal{N}(2.29, 0.076) \\
    \sigma_{\xi_{D}} &\sim \mathcal{N}(0.76, 0.055) \\
    \alpha &\sim \mathcal{N}(0.005, 0.0025) 
\end{align}

$\alpha$, $\mu$, $\sigma$, and $\phi$ were truncated to be greater than 0 and with $\xi$, and $w$ normalised to sum to 1. 

The prior for the generation time was sourced from [@generationinterval] but refit using a log-normal incubation period with a mean of 5.2 days (SD 1.1) and SD of 1.52 days (SD 1.1) with this incubation period also being used as a prior [@incubationperiod] for $\xi_{O}$. This resulted in a gamma-distributed generation time with mean 3.6 days (standard deviation (SD) 0.7), and SD of 3.1 days (SD 0.8) for all estimates. We estimated the delay between symyptom onset and case report or death required to convolve latent infections to observations by fitting an integer adjusted log-normal distribution to 10 subsampled bootstraps of a public linelist for cases in Germany from April 2020 to June 2020 with each bootstrap using 1% or 1769 samples of the available data  [@kraemer2020epidemiological; @covidregionaldata] and combining the posteriors for the mean and standard deviation of the log-normal distribution [@epinow2; @doiCovid19TemporalVariation; @EvaluatingUseReproduction; @rstan].

$GP_t$ is an approximate Hilbert space Gaussian process as defined in [@approxGP] using a Matern 3/2 kernel using a boundary factor of 1.5 and 17 basis functions (20% of the number of days used in fitting). The lengthscale of the Gaussian process was given a log-normal prior with a mean of 21 days, and a standard deviation of 7 days truncated to be greater than 3 days and less than 60 days. The magnitude of the Gaussian process was assumed be normally distributed centred at 0 with a standard deviation of 0.1.

From the forecast time horizon ($T$) and onwards the last value of the Gaussian process was used (hence $R_t$ was assumed to be fixed) and latent infections were adjusted to account for the proportion of the population that was susceptible to infection as follows,
 
\begin{equation}
    I_t = (N - I^c_{t-1}) \left(1 - \exp \left(\frac{-I'_t}{N - I^c_{T}}\right)\right),
\end{equation}

where $I^c_t = \sum_{s< t} I_s$ are cumulative infections by $t-1$ and $I'_t$ are the unadjusted infections defined above. This adjustment is based on that implemented in the `epidemia` R package (@epidemia, cite @bhatt202).

#### Convolution model

The convolution model shares the same observation model as the renewal model but rather than assuming that an observation is predicted by itself instead assumes that it is predicted entirely by another observation after some parametric delay. It can be defined mathematically as follows,

\begin{equation} 
    D_{t} \sim \mathrm{NB}\left(\omega_{(t \mod 7)} \alpha \sum_{\tau = 0}^{30} \xi(\tau | \mu, \sigma) C_{t-\tau},  \phi \right)
\end{equation}

with the following priors,

\begin{align}
    \frac{\omega}{7} &\sim \mathrm{Dirichlet}(1, 1, 1, 1, 1, 1, 1) \\
    \alpha &\sim \mathcal{N}(0.01, 0.02) \\
    \xi &\sim \mathcal{LN}(\mu, \sigma) \\
    \mu &\sim \mathcal{N}(2.5, 0.5) \\
\sigma &\sim \mathcal{N}(0.47, 0.2) \\
\phi &\sim \frac{1}{\sqrt{\mathcal{N}(0, 1)}}
\end{align}

with $\alpha$, $\mu$, $\sigma$, and $\phi$ truncated to be greater than 0 and with $\xi$ normalised such that $\sum_{\tau = 0}^{30} \xi(\tau | \mu, \sigma) = 1$. 

### Model fitting

Both models were implemented using the `EpiNow2` R package (version 1.3.3) [@epinow2]. Each forecast target was fit independently for each model using Markov-chain Monte Carlo (MCMC) in stan [@rstan]. A minimum of 4 chains were used with a warmup of 250 samples for the renewal equation-based model and 1000 samples for the convolution model. 2000 samples total post warmup were used for the renewal equation model and 4000 samples for the convolution model. Different settings were chosen for each model to optimise compute time contigent on convergence. Convergence was assessed using the R hat diagnostic [@rstan]. For the convolution model forecast the case forecast from the renewal equation model was used in place of observed cases beyond the forecast horizon using 1000 posterior samples. 12 weeks of data was used for both models though only 3 weeks of data were included in the likelihood for the convolution model.

\clearpage

```{r}
add_caption <- function(table, caption, label = NULL) {
  if (is.null(label)) {
    label <- paste0("tab:", opts_current$get("label"))
  }
  table <- gsub(pattern = "begin\\{table\\}\\[!h\\]",
                replacement = paste0("begin\\{table\\}[!h]\n", 
                                     "\\\\caption\\{",
                                     "\\\\\\label\\{", label, "\\}",
                                     caption,
                                     "\\\\\\\\\\\\hspace{\\\\textwidth}\\}\n"),
                x = table)
  return(table)
}

```


## Tables with results of the forecast evaluation 

```{r}
caption <- 
  "Scores for one and two week ahead forecasts (cut to three significant digits and rounded). Note that scores for cases (which include the whole period from October 12th 2020 until March 1st 2021) and deaths (which include only forecasts from the 21st of December 2020 on) are computed on different subsets. Numbers in brackets show the metrics relative to the Hub ensemble (i.e. the median ensemble of all other models submitted to the German and Polish Forecast Hub, excluding our contributions). WIS is the mean weighted interval score (lower values are better), WIS - sd is the standard deviation of all scores achieved by a model. Dispersion, over-prediction and under-prediction together sum up to the weighted interval score. Bias (between -1 and 1, 0 is ideal) represents the general average tendency of a model to over- or underpredict. 50\\\\% and 90\\\\%-coverage are the percentage of observed values that fell within the 50\\\\% and 90\\\\% prediction intervals of a model."
```

```{r score-table-2}
table <- readRDS("../analysis/plots/table_scores_2_ahead.rds")
add_caption(table, caption, label = "tab:score-table-2")
```

```{r}
caption <- gsub(x = caption, pattern = "one and two", replacement = "three and four")
```

```{r score-table-4}
table <- readRDS("../analysis/plots/table_scores_4_ahead.rds")
add_caption(table, caption, label = "tab:score-table-4")

```

\clearpage

## Aggregate performance by location

### Performance across locations in absolute terms
```{r}
caption <- "Visualisation of aggregate performance metrics across locations. A: mean weighted interval score (WIS) across horizons. B: median WIS. C: Absolute error of the median forecast. D: Standard deviation of the WIS. E: Sharpness (higher values mean greater dispersion of the forecast). F: Bias, i.e. general tendency to over- or underpredict. Values are between -1 (complete under-prediction) and 1 (complete over-prediction) and 0 ideally. G: Empirical coverage of the 50\\% prediction intervals. F: Empirical coverage of the 90\\% prediction intervals."
```

```{r performance-locations, fig.cap=caption}
knitr::include_graphics("../analysis/plots/aggregate-performance-2-weeks-locations-all-4.png")
```


## Performance across locations in relative terms

```{r}
caption <- " Visualisation of aggregate performance metrics across locations relative to the Hub ensemble (excluding our contributions). A: mean weighted interval score (WIS) across horizons. B: median WIS. C: Absolute error of the median forecast. D: Standard deviation of the WIS. E: Sharpness (higher values mean greater dispersion of the forecast). F: Bias, i.e. general tendency to over- or underpredict. Values are between -1 (complete under-prediction) and 1 (complete over-prediction) and 0 ideally. G: Empirical coverage of the 50\\% prediction intervals. F: Empirical coverage of the 90\\% prediction intervals."
```

```{r performance-locations-rel, fig.cap=caption}
knitr::include_graphics("../analysis/plots/aggregate-performance-2-weeks-locations-all-rel-v4.png")
```

\clearpage

## Visualisation of daily reported cases and deaths

```{r}
caption <- "Visualisation of daily report data. The black line represents weekly data divided by seven. Data were last accessed through the German and Polish Forecast Hub on August 21 2021."
```

```{r daily-truth, fig.cap=caption}
knitr::include_graphics("../analysis/plots/daily_truth.png")
```


\clearpage

## Visualisation of scores and forecasts 1, 3, 4 weeks ahead

```{r}
caption <- "A, C: Visualisation of 50\\% prediction intervals of one week ahead forecasts against the true observed values. Forecasts that were not scored (because there was no complete set of death forecasts available) are greyed out. B, D: Visualisation of corresponding WIS."
```

```{r forecasts-and-truth-1, fig.cap=caption}
knitr::include_graphics("../analysis/plots/figure-forecasts-1.png")
```


```{r}
caption <- gsub(x = caption, pattern = "one", replacement = "three")
```

```{r forecasts-and-truth-3, fig.cap=caption}
knitr::include_graphics("../analysis/plots/figure-forecasts-3.png")
```


```{r}
caption <- gsub(x = caption, pattern = "three", replacement = "four")
```

```{r forecasts-and-truth-4, fig.cap=caption}
knitr::include_graphics("../analysis/plots/figure-forecasts-4.png")
```

\clearpage

## Distribution of scores

### Absolute scores

```{r}
caption <- "A: Distribution of weighted interval scores for one week ahead forecasts of the different models and forecast targets. B: Distribution of WIS separate by country."
```

```{r distribution-scores-1, fig.cap=caption}
knitr::include_graphics("../analysis/plots/distribution_scores_wis-1.png")
```


```{r}
caption <- gsub(x = caption, pattern = "one", replacement = "three")
```

```{r distribution-scores-3, fig.cap=caption}
knitr::include_graphics("../analysis/plots/distribution_scores_wis-3.png")
```


```{r}
caption <- gsub(x = caption, pattern = "three", replacement = "four")
```

```{r distribution-scores-4, fig.cap=caption}
knitr::include_graphics("../analysis/plots/distribution_scores_wis-4.png")
```


### Ranks achieved by forecasts 

```{r}
caption <- "A: Distribution of the ranks (determined by the weighted interval score) for one week ahead forecasts of the different models and forecast targets. B: Distribution of ranks separate by country."
```

```{r distribution-scores-ranks-1, fig.cap=caption}
knitr::include_graphics("../analysis/plots/distribution_scores_wis-1-ranks.png")
```




```{r}
caption <- gsub(x = caption, pattern = "one", replacement = "two")
```

```{r distribution-scores-ranks-2, fig.cap=caption}
knitr::include_graphics("../analysis/plots/distribution_scores_wis-2-ranks.png")
```


```{r}
caption <- gsub(x = caption, pattern = "two", replacement = "three")
```

```{r distribution-scores-ranks-3, fig.cap=caption}
knitr::include_graphics("../analysis/plots/distribution_scores_wis-3-ranks.png")
```


```{r}
caption <- gsub(x = caption, pattern = "three", replacement = "four")
```

```{r distribution-scores-ranks-4, fig.cap=caption}
knitr::include_graphics("../analysis/plots/distribution_scores_wis-4-ranks.png")
```

\clearpage

## Comparison of ensembles

### Performance visualisation mean ensemble


```{r}
caption <- "Visualisation of aggregate performance metrics across forecast horizons for the different versions of the Hub mean ensemble. “Hub-ensemble” excludes all our models, Hub-ensemble-all includes all of our models, “Hub-ensemble-real” is the real hub-ensemble with the renewal model and the crowd forecasts included. Values (except for Bias) are computed as differences to the Hub ensemble excluding our contributions. For Coverage, this is an absolute difference, for other metrics this is a percentage difference. A: mean weighted interval score (WIS) across horizons. B: median WIS. C: Absolute error of the median forecast. D: Standard deviation of the WIS. E: Dispersion (higher values mean greater spread of the forecast). F: Bias, i.e. general tendency to over- or underpredict. Values are between -1 (complete under-prediction) and 1 (complete over-prediction) and 0 ideally. G: Empirical coverage of the 50\\% prediction intervals. F: Empirical coverage of the 90\\% prediction intervals"
```

```{r agg-performance-ensemble-mean, fig.cap=caption}
knitr::include_graphics("../analysis/plots/aggregate-performance-rel-ensemble-mean-v4.png")
```

### Tables median ensemble

```{r}
caption <- 
  "Scores for one and two week ahead forecasts (cut to three significant digits and rounded) for the different versions of the median ensemble. Note that scores for cases (which include the whole period from October 12th 2020 until March 1st 2021) and deaths (which include only forecasts from the 21st of December 2020 on) are computed on different subsets. Numbers in brackets show the metrics relative to the Hub ensemble (i.e. the median ensemble of all other models submitted to the German and Polish Forecast Hub, excluding our contributions). WIS is the mean weighted interval score (lower values are better), WIS - sd is the standard deviation of all scores achieved by a model. Dispersion, over-prediction and under-prediction together sum up to the weighted interval score. Bias (between -1 and 1, 0 is ideal) represents the general average tendency of a model to over- or underpredict. 50\\\\% and 90\\\\%-coverage are the percentage of observed values that fell within the 50\\\\% and 90\\\\% prediction intervals of a model."
```

```{r score-table-ensemble-2}
table <- readRDS("../analysis/plots/table_median-ensemble_scores_2_ahead.rds")
add_caption(table, caption)
```


```{r}
caption <- gsub(x = caption, pattern = "one and two", replacement = "three and four")
```

```{r score-table-ensemble-4}
table <- readRDS("../analysis/plots/table_median-ensemble_scores_4_ahead.rds")
add_caption(table, caption)

```



### Tables mean ensemble

```{r}
caption <- 
  "Scores for one and two week ahead forecasts (cut to three significant digits and rounded) for the different versions of the mean ensemble. Note that scores for cases (which include the whole period from October 12th 2020 until March 1st 2021) and deaths (which include only forecasts from the 21st of December 2020 on) are computed on different subsets. Numbers in brackets show the metrics relative to the Hub mean ensemble (i.e. the mean ensemble of all other models submitted to the German and Polish Forecast Hub, excluding our contributions). WIS is the mean weighted interval score (lower values are better), WIS - sd is the standard deviation of all scores achieved by a model. Dispersion, over-prediction and under-prediction together sum up to the weighted interval score. Bias (between -1 and 1, 0 is ideal) represents the general average tendency of a model to over- or underpredict. 50\\\\% and 90\\\\%-coverage are the percentage of observed values that fell within the 50\\\\% and 90\\\\% prediction intervals of a model."
```

```{r score-table-ensemble-mean-2}
table <- readRDS("../analysis/plots/table_mean-ensemble_scores_2_ahead.rds")
add_caption(table, caption)
```


```{r}
caption <- gsub(x = caption, pattern = "one and two", replacement = "three and four")
```

```{r score-table-ensemble-mean-4}
table <- readRDS("../analysis/plots/table_mean-ensemble_scores_4_ahead.rds")
add_caption(table, caption)
```

\clearpage

## Sensitivity analysis

In the original analysis, cases and deaths were scored on different periods, as the convolution model was only added later. This sensitivity shows performance of all models restricted to the period from October 14 2020 until March 1st 2021 where all models were available. 

```{r}
caption <- "Visualisation of aggregate performance metrics across forecast horizons only for the period from October 14th 2020 on where all models were available. A, B: mean weighted interval score (WIS, lower indicates better performance) across horizons. WIS is decomposed into its components dispersion, over-prediction and under-prediction. C: Empirical coverage of the 50\\% prediction intervals (50\\% coverage is perfect). D: Empirical coverage of the 90\\% prediction intervals. E: Dispersion (same as in panel A, B). Higher values mean greater dispersion of the forecast and imply ceteris paribus a worse score. F: Bias, i.e. general (relative) tendency to over- or underpredict. Values are between -1 (complete under-prediction) and 1 (complete over-prediction) and 0 ideally. G: Absolute error of the median forecast (lower is better). H. Standard deviation of all WIS values for different horizons"
```

```{r agg-performance-all-late, fig.cap=caption}
knitr::include_graphics("../analysis/plots/aggregate-performance-all-late-period-v4.png")
```



```{r}
caption <- 
  "Scores for one and two week ahead forecasts (cut to three significant digits and rounded) calculated on forecasts made between December 14th 2020 and March 1st 2021. Numbers in brackets show the metrics relative to the Hub ensemble (i.e. the median ensemble of all other models submitted to the German and Polish Forecast Hub, excluding our contributions). WIS is the mean weighted interval score (lower values are better), WIS - sd is the standard deviation of all scores achieved by a model. Dispersion, over-prediction and under-prediction together sum up to the weighted interval score. Bias (between -1 and 1, 0 is ideal) represents the general average tendency of a model to over- or underpredict. 50\\\\% and 90\\\\%-coverage are the percentage of observed values that fell within the 50\\\\% and 90\\\\% prediction intervals of a model."
```

```{r score-table-late-2}
table <- readRDS("../analysis/plots/table-late-period_2_ahead.rds")
add_caption(table, caption)
```

```{r}
caption <- gsub(x = caption, pattern = "one and two", replacement = "three and four")
```

```{r score-table-late-4}
table <- readRDS("../analysis/plots/table-late-period_4_ahead.rds")
add_caption(table, caption)

```


\clearpage

## Overview of models and forecasters

```{r table-ensemble-versions}

rbind(
  list(
    Name = "Hub-ensemble-realised", 
    `Explanation` = "Official Forecast Hub median ensemble. Created by the Forecast Hub officially under the name 'KITCOVIDhub-median\\_ensemble' and used as the default ensemble. Included are our crowd forecasts as well as the renewal model (with one missed submission on December 28 2020, but not the convolution model which was deemed to similar to the renewal model."
  ),
  list(
    Name = "Hub-ensemble-realised-mean", 
    `Explanation` = "Official Forecast Hub mean ensemble. Created by the Forecast Hub officially under the name 'KITCOVIDhub-mean\\_ensemble'."
  ),
  list(Name = "", Explanation = ""),
  
  list(
    Name = "Hub-ensemble", 
    `Explanation` = "Version of the official Hub median ensemble which excludes all our contributions."
  ),
  list(
    Name = "Hub-ensemble-mean", 
    `Explanation` = "Version of the official Hub mean ensemble which excludes all our contributions."
  ),
  list(
    Name = "Hub-ensemble-with-renewal, 
    Hub-ensemble-with-renewal-mean", 
    `Explanation` = "Versions of the official Hub ensembles which of our contributions includes only the Renewal model."
  ),
  list(
    Name = "Hub-ensemble-with-crowd, 
    Hub-ensemble-with-crowd-mean", 
    `Explanation` = "Versions of the official Hub ensembles which of our contributions includes only the Crowd forecast."
  ),
  list(
    Name = "Hub-ensemble-with-convolution, 
    Hub-ensemble-with-convolution-mean", 
    `Explanation` = "Versions of the official Hub ensembles which of our contributions includes only the Convolution model (which originally was never included in any official Hub ensemble)."
  ),
    list(
    Name = "Hub-ensemble-with-all, 
    Hub-ensemble-with-all-mean", 
    `Explanation` = "Versions of the official Hub ensembles which includes all our contributions. For cases, this is identical to the official Hub ensembles, but for deaths the convolution model was added."
  ),
  
  list(Name = "", Explanation = ""),
  list(
    Name = "Crowd forecast", 
    `Explanation` = "Submitted to the Forecast Hub as 'epiforecasts-EpiExpert'"
  ),  
  list(
    Name = "Renewal model", 
    `Explanation` = "Submitted to the Forecast Hub as 'epiforecasts-EpiNow2'"
  ),  
  list(
    Name = "Convolution model", 
    `Explanation` = "Submitted to the Forecast Hub as 'epiforecasts-EpiNow2\\_secondary'"
  )
) %>%
  kableExtra::kbl(format = "latex", booktabs = TRUE, 
                  escape = FALSE,
                  caption = "Overview of the models and ensembles used.",
                  longtable = TRUE,
                  linesep = c('\\addlinespace \\addlinespace')) %>%
  kableExtra::column_spec(1, width = "4.5cm") %>%
  kableExtra::column_spec(2, width = "11.0cm") %>%
  kableExtra::kable_styling(latex_options = c("striped", "repeat_header")) 
```



```{r num-forecasters, fig.cap="Number of participants who submitted a forecast over time."}
knitr::include_graphics("../analysis/plots/number-forecasters.png")
```


```{r num-ensemble-members, fig.cap="Number of member models (including our crowd forecasts and the renewal model) in the official Hub ensemble. Note that the renewal model was not included in the ensemble on December 28th 2020."}
knitr::include_graphics("../analysis/plots/ensemble-members.png")
```

\clearpage
